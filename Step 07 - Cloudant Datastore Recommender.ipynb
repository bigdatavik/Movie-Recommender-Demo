{
    "nbformat_minor": 0, 
    "cells": [
        {
            "source": "**Important:** Before running this notebook, ensure you have installed spark-cloudant 1.6.4 by running the notebook: **Step 05 - Install Spark Cloudant**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Next, add your cloudant credentials below, delete the hash before the 'echo' command and run the cell to save your credentials", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "! # echo '{ \"username\": \"changeme\", \"password\": \"changeme\", \"host\": \"changeme\", \"port\": 443, \"url\": \"changeme\" }' > cloudant_credentials.json", 
            "execution_count": 2, 
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "! echo '{ \"username\": \"875bb6df-554d-48d3-a8b2-f86cb2ce56cc-bluemix\", \"password\": \"ed9427b45762a35745d176743d9dede76c8b5ff0dfbc4a95890c6a8e56a6d124\", \"host\": \"875bb6df-554d-48d3-a8b2-f86cb2ce56cc-bluemix.cloudant.com\", \"port\": 443, \"url\": \"https://875bb6df-554d-48d3-a8b2-f86cb2ce56cc-bluemix:ed9427b45762a35745d176743d9dede76c8b5ff0dfbc4a95890c6a8e56a6d124@875bb6df-554d-48d3-a8b2-f86cb2ce56cc-bluemix.cloudant.com\" }' > cloudant_credentials.json", 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "! python -c 'import cloudant' || pip install cloudant --user\n", 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# utility method for timestamps\nimport time\ndef ts():\n    return time.strftime(\"%Y-%m-%d %H:%M:%S %Z\")", 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# utility method for logging\nlog4jLogger = sc._jvm.org.apache.log4j\nLOGGER = log4jLogger.LogManager.getLogger(\"CloudantRecommender\")\n\ndef info(*args):\n    \n    # sends output to notebook\n    print(args)\n    \n    # sends output to kernel log file\n    LOGGER.info(args)\n    \ndef error(*args):\n    \n    # sends output to notebook\n    print(args)\n    \n    # sends output to kernel log file\n    LOGGER.error(args)", 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# utility class for holding cloudant connection details\nimport json\n\ndef set_attr_if_exists(obj, data, k):\n    try:\n        setattr(obj, k, data[k])\n    except AttributeError:\n        pass\n\nclass CloudantConfig:\n    def __init__(self, database, json_file=None, host=None, username=None, password=None):\n       \n        self.database = database\n        self.host = None\n        self.username = None\n        self.password = None\n\n        with open(json_file) as data_file:    \n            data = json.load(data_file)\n            \n            set_attr_if_exists(self, data, 'host')\n            set_attr_if_exists(self, data, 'username')\n            set_attr_if_exists(self, data, 'password')\n        \n        # override json attributes if provided\n        if host:     self.host = host\n        if username: self.username = username\n        if password: self.password = password", 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "sourceDB = CloudantConfig(\n                    json_file='cloudant_credentials.json', \n                    database=\"ratingdb\"\n                    )", 
            "execution_count": 9, 
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": " - We generate recommendations, create a new Cloudant database for the recommendations and save them into the new Cloudant database.\n - When we have finished writing the recommendations to Cloudant, we save a metadata record into the recommendation_meta database with the name of the new database.\n - Client applications use the metadata record to determine which database to retrieve the recommendations from.\n - We delete older databases after writing the metadata, but keep the five latest ones.\n - We need to keep at least one database because if a client reads the meta pointing to the previous database it will try to read from that database.\n - We don't have just one database and continually update the recommendation records in Cloudant because lots of changes can be considered an anti-pattern.\n - The recommendation_meta database is created for us by the web application setup scripts.\n - The spark-cloudant package is used to read the data from Cloudant but not to write the data to Cloudant because of this issue: https://github.com/cloudant-labs/spark-cloudant/issues/82\n - The python-cloudant package is used to write the data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n\nimport json\nimport numpy as np\n\n# we use the cloudant python library to save the recommendations\nfrom cloudant.client import Cloudant\nfrom cloudant.adapters import Replay429Adapter\n\nclass CloudantMovieRecommender:\n    \n    def __init__(self, sc):\n        self.sc = sc\n    \n    def train(self, sourceDB):\n                      \n        info(\"Starting load from Cloudant: \", ts())\n\n        dfReader = sqlContext.read.format(\"com.cloudant.spark\")\n        dfReader.option(\"cloudant.host\", sourceDB.host)\n        \n        if sourceDB.username:\n            dfReader.option(\"cloudant.username\", sourceDB.username)\n            \n        if sourceDB.password:\n            dfReader.option(\"cloudant.password\", sourceDB.password)\n            \n        df = dfReader.load(sourceDB.database).cache()\n\n        info(\"Finished load from Cloudant: \", ts())\n        info(\"Found\", df.count(), \"records in Cloudant\")\n        \n        # convert cloudant docs into Rating objects\n        def make_rating(row):\n            (user_id, prod_id) = row[0].split('/')\n            user_id = int(user_id.replace('user_', ''))\n            prod_id = int(prod_id.replace('movie_', ''))\n\n            rating = float(row[2])\n            return Rating(user_id, prod_id, rating)\n        \n        ratings = df.map(make_rating)\n\n        rank = 50\n        numIterations = 20\n        lambdaParam = 0.1\n\n        info(\"Starting train model: \", ts())\n        self.model = ALS.train(ratings, rank, numIterations, lambdaParam)\n        info(\"Finished train model: \", ts())\n        \n    def get_top_recommendations(self):\n        info(\"Starting __get_top_recommendations: \", ts())\n        df = self.model.recommendProductsForUsers(10).toDF()\n        df.cache()\n        info(\"Finished __get_top_recommendations: \", ts())\n        return df\n        \n    def del_old_recommendationdbs(self, cloudant_client, db_name_prefix):\n        dbs_to_del = cloudant_client.all_dbs()\n\n        # only delete dbs we are using for recommendations\n        dbs_to_del = [db for db in dbs_to_del if db.startswith(db_name_prefix + '_') ]\n\n        # ensure the list is in timestamp order\n        dbs_to_del.sort()\n\n        # keeping the last 5 dbs and delete the rest\n        for db in dbs_to_del[:-5]:\n            cloudant_client.delete_database(db)\n            info(\"Deleted old recommendations db\", db)\n            \n    def update_meta_document(self, cloudant_client, meta_db_name, latest_db_name):\n        \n        meta_db = cloudant_client[meta_db_name]\n        \n        from datetime import datetime\n        ts = datetime.utcnow().isoformat()\n\n        try:\n            # update doc if exists\n            meta_doc = meta_db['recommendation_metadata']\n            meta_doc['latest_db'] = latest_db_name\n            meta_doc['timestamp_utc'] = ts\n            meta_doc.save()\n            info(\"Updated recommendationdb metadata record with latest_db\", latest_db_name, meta_doc)\n        except KeyError:\n            # create a new doc\n            data = {\n                '_id': 'recommendation_metadata',\n                'latest_db': latest_db_name,\n                'timestamp_utc': ts,\n                }\n            meta_doc = meta_db.create_document(data)\n            meta_doc.save()\n            \n            if meta_doc.exists():\n                info(\"Saved recommendationdb metadata record\", str(data))\n                \n        # save product features to enable later generationg of Vt\n        # see: http://stackoverflow.com/questions/41537470/als-model-how-to-generate-full-u-vt-v\n        pf = self.model.productFeatures().sortByKey()\n\n        pf_keys = json.dumps(pf.sortByKey().keys().collect())\n        pf_vals = json.dumps(pf.sortByKey().map(lambda x: list(x[1])).collect())               \n        \n        # the pf_keys/pf_vals are too big and exceed the >1mb document size limit\n        # so we save them as attachments\n        \n        meta_doc.put_attachment(\n            attachment='product_feature_keys', \n            content_type='application/json', \n            data=pf_keys\n        )\n\n        meta_doc.put_attachment(\n            attachment='product_feature_vals', \n            content_type='application/json', \n            data=pf_vals\n        )\n    \n    def create_recommendationdb(self, cloudant_client):\n        # create a database for recommendations\n        import time\n        db_name = destDB.database + '_' + str(int(time.time()))\n        \n        db = cloudant_client.create_database(db_name)\n        info(\"Created new recommendations db\", db_name)\n        return db\n        \n    def save_recommendations(self, destDB):\n        df = movieRecommender.get_top_recommendations()\n        \n        cloudant_client = Cloudant(\n                                destDB.username,\n                                destDB.password,\n                                account=destDB.username, \n                                adapter=Replay429Adapter(retries=10, initialBackoff=1)\n                                )\n        cloudant_client.connect()\n        self.del_old_recommendationdbs(cloudant_client, destDB.database)\n        recommendations_db = self.create_recommendationdb(cloudant_client)\n\n        # reformat data for saving\n        docs = df.map(lambda x: {'_id':str(x[0]), 'recommendations':x[1]}).collect()\n        \n        # we could hit cloudant resource limits if trying to save entire doc\n        # so we save it in smaller sized chunks\n        \n        for i in range(0, len(docs), 100):\n            chunk = docs[i:i + 100]\n            recommendations_db.bulk_docs(chunk) # TODO check for errors saving the chunk\n            info(\"Saved recommendations chunk\", i, ts())\n        \n        self.update_meta_document(cloudant_client, destDB.database, recommendations_db.database_name)\n        \n        info(\"Saved recommendations to: \", recommendations_db.database_name, ts())\n\n        cloudant_client.disconnect()", 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "Now the code to start the recommender ...", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "sourceDB = CloudantConfig(\n                    json_file='cloudant_credentials.json', \n                    database=\"ratingdb\"\n                    )\n\ndestDB = CloudantConfig(\n                    json_file='cloudant_credentials.json', \n                    database=\"recommendationdb\", \n                    )\n\nimport traceback\ntry:\n    movieRecommender = CloudantMovieRecommender(sc)\n    movieRecommender.train(sourceDB)\n    movieRecommender.save_recommendations(destDB)\nexcept Exception as e:\n    error(str(e), traceback.format_exc(), ts())\n    raise e", 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "('Starting load from Cloudant: ', '2017-01-19 16:37:35 CST')\n('Finished load from Cloudant: ', '2017-01-19 16:38:27 CST')\n('Found', 1000000, 'records in Cloudant')\n('Starting train model: ', '2017-01-19 16:39:00 CST')\n('Finished train model: ', '2017-01-19 16:39:49 CST')\n('Starting __get_top_recommendations: ', '2017-01-19 16:39:49 CST')\n('Finished __get_top_recommendations: ', '2017-01-19 16:39:59 CST')\n('Created new recommendations db', 'recommendationdb_1484865599')\n('Saved recommendations chunk', 0, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 100, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 200, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 300, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 400, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 500, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 600, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 700, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 800, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 900, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 1000, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 1100, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 1200, '2017-01-19 16:40:01 CST')\n('Saved recommendations chunk', 1300, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 1400, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 1500, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 1600, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 1700, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 1800, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 1900, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 2000, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 2100, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 2200, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 2300, '2017-01-19 16:40:03 CST')\n('Saved recommendations chunk', 2400, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 2500, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 2600, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 2700, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 2800, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 2900, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 3000, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 3100, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 3200, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 3300, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 3400, '2017-01-19 16:40:06 CST')\n('Saved recommendations chunk', 3500, '2017-01-19 16:40:08 CST')\n('Saved recommendations chunk', 3600, '2017-01-19 16:40:08 CST')\n('Saved recommendations chunk', 3700, '2017-01-19 16:40:08 CST')\n('Saved recommendations chunk', 3800, '2017-01-19 16:40:08 CST')\n('Saved recommendations chunk', 3900, '2017-01-19 16:40:08 CST')\n('Saved recommendations chunk', 4000, '2017-01-19 16:40:08 CST')\n('Saved recommendations chunk', 4100, '2017-01-19 16:40:08 CST')\n('Saved recommendations chunk', 4200, '2017-01-19 16:40:08 CST')\n('Saved recommendations chunk', 4300, '2017-01-19 16:40:08 CST')\n('Saved recommendations chunk', 4400, '2017-01-19 16:40:08 CST')\n('Saved recommendations chunk', 4500, '2017-01-19 16:40:09 CST')\n('Saved recommendations chunk', 4600, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 4700, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 4800, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 4900, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 5000, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 5100, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 5200, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 5300, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 5400, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 5500, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 5600, '2017-01-19 16:40:11 CST')\n('Saved recommendations chunk', 5700, '2017-01-19 16:40:13 CST')\n('Saved recommendations chunk', 5800, '2017-01-19 16:40:13 CST')\n('Saved recommendations chunk', 5900, '2017-01-19 16:40:13 CST')\n('Saved recommendations chunk', 6000, '2017-01-19 16:40:13 CST')\n('Saved recommendationdb metadata record', \"{'timestamp_utc': '2017-01-19T22:40:13.807491', '_id': 'recommendation_metadata', 'latest_db': 'recommendationdb_1484865599'}\")\n('Saved recommendations to: ', 'recommendationdb_1484865599', '2017-01-19 16:40:15 CST')\n", 
                    "output_type": "stream"
                }
            ], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "## Scheduling", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "After you have successfully run the notebook interactively you can schedule your notebook to run on a timer, for example hourly.\n\nTimer jobs work again a specific saved version of a notebook.  If you haven't saved a version when you first create the schedule a version will be saved for you.  \n\nNote that if you make changes to your notebook, you need to save a new version and re-schedule the notebook with the new version of the notebook selected in the schedule configuration form.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## For debugging issues", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Get the local time on the cluster\n! date", 
            "execution_count": 12, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Thu Jan 19 16:40:15 CST 2017\r\n", 
                    "output_type": "stream"
                }
            ], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# dump the latest kernel log\n! cat $(ls -1 $HOME/logs/notebook/*pyspark* | sort -r | head -1)", 
            "execution_count": 13, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "========================== 20170119_222801 ==========================\n/usr/local/src/bluemix_jupyter_bundle.v31/provision/pyspark_kernel_wrapper.sh /gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/jupyter-rt/kernel-007a4a4d-b367-4725-8806-e3fdc4ddc222.json spark160master\nno extra config\nload default config from : /gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/spark-config/spark160master\n-------- Environment for PySpark --------\nAPP_ENV_BM_DOMAIN=ng.bluemix.net\nAPP_ENV_CDSX_NOTEBOOKS_API=cdsx-notebooks-api.ng.bluemix.net\nAPP_ENV_ENVIRONMENT=prod\nAPP_ENV_IBM_ONLY_AUTH=false\nAPP_ENV_JUPYTER_TENANTS_API=cdsx-tenants-api.ng.bluemix.net\nAPP_ENV_NOTEBOOKS_JOB_MANAGER=cdsx-notebooks-job-manager.ng.bluemix.net\nATLAS_VERSION=3.10.2\n_=/bin/printenv\nBLUEMIX_RES_PLAN=s\nBRUNEL_CONFIG=locjavascript=/data/jupyter2/446fc60b-abc3-4da3-8c07-a3c399a7caae/nbextensions/brunel_ext\nCC_DISABLE_BIG_BUFFER_API=true\nCDSX_APP_ENV_NOTEBOOKS_API_URL=https://cdsx-notebooks-api.ng.bluemix.net/v1/notebooks/\nC_INCLUDE_PATH=/usr/local/src/bluemix_jupyter_bundle.v31/notebook/include:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/include:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/include:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/include:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/include:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/include:\nDEPLOY_HOME=/usr/local/src/bluemix_jupyter_bundle.v31\nDS_RESOURCE_LOCATION=/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/res\nDW_CONN_HOME=/usr/local/src/dataconnector-dw/spark-1.6.0/Server\nEGO_ACTIVITY_PID=-1\nEGO_CONFDIR=/disk1/ego/kernel/conf\nEGO_CONTAINER_ID=967137\nEGO_EXPORT_OS_USER_ENV=N\nEGO_KD_PORT=7870\nEGO_LIBDIR=/disk1/ego/3.4/linux-x86_64/lib\nEGO_LOAD_BASHRC_SRC=N\nEGO_MASTER_LIST_PEM=yp-spark-dal09-env5-0001.bluemix.net yp-spark-dal09-env5-0003\nEGOSC_INSTANCE_HOST=yp-spark-dal09-env5-0027\nEGOSC_INSTANCE_INDEX_PER_HOST=1\nEGOSC_INSTANCE_RESOURCE_GROUP=ComputeHosts\nEGOSC_INSTANCE_SEQNO=1\nEGOSC_INSTANCE_START_REASON=FirstStart\nEGOSC_SERVICE_NAME=sc07-a3c399a7caae2d-99fc3133bdbb\nHOME=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb\nIBM_JAVA_OPTIONS=-Dderby.system.home=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/jupyter-rt/kernel-007a4a4d-b367-4725-8806-e3fdc4ddc222-20170119_222801\nJAR_DIR=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/downloads\nJAVA_HOME=/usr/local/src/spark160master/ibm-java-x86_64-80\nJPY_PARENT_PID=17694\nJUPYTER_CONFIG_DIR=/usr/local/src/bluemix_jupyter_bundle.v31/provision/jupyter-ax-ext\nJUPYTER_DATA_DIR=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/jupyter-data\nJUPYTER_RUNTIME_DIR=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/jupyter-rt\nKERNEL_ACTIVITY_LOG=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/process-info/kernel_activity.status\nLANG=en_US.UTF-8\nLD_LIBRARY_PATH=/usr/local/lib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/daapi-sca:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/C/icc/icclib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/C/icc/osslib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/N/icc/icclib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/N/icc/osslib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/daapi-xml:/usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/amd64/compressedrefs:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:\nLIBRARY_PATH=/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:\nLOG_FILE=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/logs/kernel-pyspark-20170119_222801.log\nMASTER=spark://yp-spark-dal09-env5-0027:7082\nNGWB_TAM_FILE_LOCATION=/usr/local/src/analytic-libs/spark-1.6.0/tam\nNOTEBOOK_HOME=/usr/local/src/bluemix_jupyter_bundle.v31/notebook\nNOTEBOOK_KERNEL=python2\nNOTEBOOK_TENANT_ID=446fc60b-abc3-4da3-8c07-a3c399a7caae\nPATH=/usr/local/src/scala/2.10/bin:/usr/local/src/spark160master/ibm-java-x86_64-80/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/bin:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/bin:/bin\nPIP_DISABLE_PIP_VERSION_CHECK=true\nPIP_USER=true\nPWD=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/work\n_py_cads_dir=/usr/local/src/cognitive-assistant/cads_packages\n_py_dir_=/usr/local/src/analytic-libs/python\n_py_spark_dir_=/usr/local/src/analytic-libs/spark-1.6.0/python\nPYSPARK_DRIVER_PYTHON_OPTS=-m ipykernel -f \"/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/jupyter-rt/kernel-007a4a4d-b367-4725-8806-e3fdc4ddc222.json\"\nPYSPARK_PYTHON=/usr/local/src/bluemix_jupyter_bundle.v31/notebook/bin/python\nPYSPARK_SUBMIT_ARGS=--master \"spark://yp-spark-dal09-env5-0027:7082\" --jar-dir \"/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/downloads\"\nPYTHONPATH=/usr/local/src/bluemix_jupyter_bundle.v31/provision/jupyter-ax-ext/cdsax_jupyter_extensions.egg:/usr/local/src/bluemix_jupyter_bundle.v31/provision/site-python::/usr/local/src/analytic-libs/python-2.7:/usr/local/src/analytic-libs/python:/usr/local/src/cognitive-assistant/cads_packages:/usr/local/src/analytic-libs/spark-1.6.0/python-2.7:/usr/local/src/analytic-libs/spark-1.6.0/python\n_py_version_=2.7\nR_HOME_PREFIX=/usr/local/src/bluemix_jupyter_bundle.v31\nR_LIBS_USER=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/R/libs\nRUNTIME_ENV_NOTEBOOK=prod\nRUNTIME_ENV_PAAS=sl\nRUNTIME_ENV_SPARK=prod\nRUNTIME_ENV_STOREFRONT=bluemix/prod\nSCALA_HOME=/usr/local/src/scala/2.10\nSERVICE_CALLER=AX\nSERVICE_HOME=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook\nSHLVL=2\nSPARK_CONF_DIR=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/spark-config/spark160master\nSPARK_CONFIG_HOME=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/spark-config\nSPARK_DEPLOY_RESOURCE_SCHEDULER=ego\nSPARK_DIST_CLASSPATH=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/data/libs/scala-2.10/*:/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/data/libs:/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/data/libs/*:/usr/local/src/dataconnector-s3/spark-1.6.0/libs/*:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/*:/usr/local/src/dataconnector-cloudant/*:/usr/local/src/analytic-libs/spark-1.6.0/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jars/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jdbc/lib/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/*:/usr/local/src/dataconnector-dw/spark-1.6.0/libs/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/aws/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/config:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/FaspStreamSDK/lib/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/jars/JISPlugins/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/branded_jdbc/lib/*:/usr/local/src/dataconnector-dw/spark-1.6.0/ASBServer/apps/lib/iis/*/*:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/connectors/*/*:/usr/local/src/event-stream/spark-1.6.0/libs/*:/usr/local/src/dataconnector-db2/*\nSPARK_DRIVER_MEMORY=1512M\nSPARK_EGO_APP_SCHEDULE_POLICY=hierarchy\nSPARK_EGO_CLASSPATH=/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/avro-1.8.0.jar:\nSPARK_EGO_CLIENT_TIMEOUT=1200\nSPARK_EGO_CONSUMER=/SparkOnBluemix/sc07-a3c399a7caae2d-99fc3133bdbb\nSPARK_EGO_DRIVER_CONSUMER=/SparkDrivers\nSPARK_EGO_DRIVER_PLAN=ComputeHosts\nSPARK_EGO_EXECUTOR_CONSUMER=/SparkOnBluemix/Spark160MasterLow/*\nSPARK_EGO_EXECUTOR_IDLE_TIMEOUT=600\nSPARK_EGO_EXECUTOR_PLAN=ComputeHosts\nSPARK_EGO_EXECUTOR_SLOTS_MAX=3\nSPARK_EGO_HIERARCHY_CONF_FILE=/usr/local/src/spark160master/spark/profile/notebook/tag.json\nSPARK_EGO_IPYTHON=true\nSPARK_EGO_JARS=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/spark-config/spark160master:/usr/local/src/spark160master/spark/ego/spark-launcher_2.10-1.6.0.jar:/usr/local/src/spark160master/spark/ego/spark-network-shuffle_2.10-1.6.0.jar:/usr/local/src/spark160master/spark/ego/gson-2.2.4.jar:/usr/local/src/spark160master/spark/ego/guava-14.0.1.jar:/usr/local/src/spark160master/spark/ego/Java-WebSocket-1.3.0.jar:/usr/local/src/spark160master/spark/ego/spark-ego_2.10-1.6.0.jar\nSPARK_EGO_LOG_DIR=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/spark/executor\nSPARK_EGO_NATIVE_LIBRARY=/usr/local/src/spark160master/spark/ego/libSparkVEMApi.so\nSPARK_EGO_RUN_AS_SERVICE=true\nSPARK_EGO_STAGING_DIR=/tmp/spark-160-ego-master/staging\nSPARK_EGO_TAG_PATH=/root/s/sc07-a3c399a7caae2d-99fc3133bdbb\nSPARK_ENV_LOADED=1\nSPARK_EXECUTOR_MEMORY=6G\nSPARK_EXECUTOR_OPTS=-Dspark.shuffle.service.port=7340\nSPARK_HISTORY_DATA=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/events\nSPARK_HISTORY_LOG=/gpfs/fs01/spark-ego-master\nSPARK_HISTORY_OPTS= -Dspark.eventLog.enabled=true -Dspark.eventLog.dir=/gpfs/fs01/spark-ego-master -Dspark.history.fs.logDirectory=/gpfs/fs01/spark-ego-master\nSPARK_HOME=/usr/local/src/spark160master/spark\nSPARK_LOCAL_DIRS=/tmp/spark-160-ego-master/work\nSPARK_LOG_DIR=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/logs\nSPARK_MASTER_IP=yp-spark-dal09-env5-0027\nSPARK_MASTER_PORT=7082\nSPARK_MASTER_WEBUI_PORT=12024\nSPARK_PACKAGE=spark160master\nSPARK_PID_DIR=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/process-ids\nSPARK_SCALA_VERSION=2.10\nSPARK_SERVICE_NAME=Spark-for-dsx\nSPARK_SUBMIT_CLASSPATH=/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/avro-1.8.0.jar:\nSPARK_SUBMIT_LIBRARY_PATH=/usr/local/lib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/daapi-sca:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/C/icc/icclib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/C/icc/osslib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/N/icc/icclib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/N/icc/osslib:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/lib/daapi-xml:/usr/local/src/spark160master/ibm-java-x86_64-80/jre/lib/amd64/compressedrefs:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/system/lib:/usr/local/src/bluemix_jupyter_bundle.v31/notebook/freetype/lib:\nSPARK_TENANT_ID=sc07-a3c399a7caae2d-99fc3133bdbb\nSPARK_WORK_DIR=/tmp/spark-160-ego-master/work\nTEMP=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/tmp\nTMPDIR=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/tmp\nTMP=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/tmp\nUSER=sc07-a3c399a7caae2d-99fc3133bdbb\nVCAP_SERVICES={}\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/src/spark160master/spark-1.6.0-bin-2.6.0/lib/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/src/dataconnector-stocator-1.6/spark-1.6.0/libs/slf4j-log4j12-1.7.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/src/analytic-libs/spark-1.6.0/tika-app-1.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/src/dataconnector-dw/spark-1.6.0/Server/connectivity/thirdparty/slf4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n17/01/19 16:28:04 INFO apache.spark.SparkContext: Running Spark version 1.6.0\n17/01/19 16:28:04 WARN hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/01/19 16:28:05 INFO apache.spark.SparkContext: Spark configuration:\nspark.app.name=PySparkShell\nspark.deploy.resourceScheduler.factory=org.apache.spark.deploy.master.EGOResourceSchedulerFactory\nspark.driver.maxResultSize=1210M\nspark.driver.memory=1512M\nspark.eventLog.dir=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/events\nspark.eventLog.enabled=true\nspark.executor.extraJavaOptions=-Djava.security.egd=file:/dev/./urandom\nspark.executor.memory=6G\nspark.history.fs.logDirectory=/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/events\nspark.logConf=true\nspark.master=spark://yp-spark-dal09-env5-0027:7082\nspark.port.maxRetries=512\nspark.r.command=/usr/local/src/bluemix_jupyter_bundle.v31/R/bin/Rscript\nspark.rdd.compress=True\nspark.serializer.objectStreamReset=100\nspark.shuffle.service.enabled=true\nspark.shuffle.service.port=7340\nspark.sql.tungsten.enabled=false\nspark.sql.unsafe.enabled=false\nspark.submit.deployMode=client\nspark.task.maxFailures=10\nspark.ui.enabled=false\nspark.ui.retainedJobs=0\nspark.ui.retainedStages=0\nspark.worker.ui.retainedExecutors=0\n17/01/19 16:28:05 INFO apache.spark.SecurityManager: Changing view acls to: sc07-a3c399a7caae2d-99fc3133bdbb\n17/01/19 16:28:05 INFO apache.spark.SecurityManager: Changing modify acls to: sc07-a3c399a7caae2d-99fc3133bdbb\n17/01/19 16:28:05 INFO apache.spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sc07-a3c399a7caae2d-99fc3133bdbb); users with modify permissions: Set(sc07-a3c399a7caae2d-99fc3133bdbb)\n17/01/19 16:28:05 INFO spark.util.Utils: Successfully started service 'sparkDriver' on port 35149.\n17/01/19 16:28:05 INFO apache.spark.SparkEnv: The address of rpcenv is :10.143.133.233:35149\n17/01/19 16:28:06 INFO event.slf4j.Slf4jLogger: Slf4jLogger started\n17/01/19 16:28:06 INFO Remoting: Starting remoting\n17/01/19 16:28:06 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.143.133.233:36437]\n17/01/19 16:28:06 INFO spark.util.Utils: Successfully started service 'sparkDriverActorSystem' on port 36437.\n17/01/19 16:28:06 INFO apache.spark.SparkEnv: Registering MapOutputTracker\n17/01/19 16:28:06 INFO apache.spark.SparkEnv: Registering BlockManagerMaster\n17/01/19 16:28:06 INFO spark.storage.DiskBlockManager: Created local directory at /tmp/spark-160-ego-master/work/blockmgr-709e3759-5c95-456b-976f-a3b37c546046\n17/01/19 16:28:06 INFO spark.storage.MemoryStore: MemoryStore started with capacity 909.0 MB\n17/01/19 16:28:06 INFO apache.spark.SparkEnv: Registering OutputCommitCoordinator\n17/01/19 16:28:06 INFO spark.util.EGOSparkDockerConfig: Docker not enabled\n17/01/19 16:28:06 INFO cluster.ego.EGOFineGrainedSchedulerBackend: setting reserve=0, priority=1, limit=2147483647,  master=spark://yp-spark-dal09-env5-0027:7082\n17/01/19 16:28:06 INFO client.ego.EGOAppClient$ClientEndpoint: Connecting to master spark://yp-spark-dal09-env5-0027:7082...\n17/01/19 16:28:06 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Connected to Spark cluster with app ID app-20170119162806-0840-6108780c-85d0-4966-a3da-83e22e6b4035\n17/01/19 16:28:06 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Application registered successfully as app-20170119162806-0840-6108780c-85d0-4966-a3da-83e22e6b4035\n17/01/19 16:28:06 INFO spark.util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45916.\n17/01/19 16:28:06 INFO network.netty.NettyBlockTransferService: Server created on 45916\n17/01/19 16:28:06 INFO spark.storage.BlockManager: external shuffle service port = 7340\n17/01/19 16:28:06 INFO spark.storage.BlockManagerMaster: Trying to register BlockManager\n17/01/19 16:28:06 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager 10.143.133.233:45916 with 909.0 MB RAM, BlockManagerId(driver, 10.143.133.233, 45916)\n17/01/19 16:28:06 INFO spark.storage.BlockManagerMaster: Registered BlockManager\n17/01/19 16:28:07 INFO spark.scheduler.EventLoggingListener: Logging events to file:/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/events/app-20170119162806-0840-6108780c-85d0-4966-a3da-83e22e6b4035\n17/01/19 16:28:07 INFO cluster.ego.EGODeployScheduler: Spark context initialized.\n17/01/19 16:37:35 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-19 16:37:35 CST]\n17/01/19 16:37:35 INFO sql.hive.HiveContext: Initializing execution hive, version 1.2.1\n17/01/19 16:37:35 INFO hive.client.ClientWrapper: Inspected Hadoop version: 2.6.0\n17/01/19 16:37:35 INFO hive.client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0\n17/01/19 16:37:36 INFO hive.metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore\n17/01/19 16:37:36 INFO hive.metastore.ObjectStore: ObjectStore, initialize called\n17/01/19 16:37:36 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n17/01/19 16:37:36 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored\n17/01/19 16:37:38 INFO hive.metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n17/01/19 16:37:39 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n17/01/19 16:37:39 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n17/01/19 16:37:40 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n17/01/19 16:37:40 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n17/01/19 16:37:40 INFO hive.metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY\n17/01/19 16:37:40 INFO hive.metastore.ObjectStore: Initialized ObjectStore\n17/01/19 16:37:40 WARN hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n17/01/19 16:37:40 WARN hive.metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException\n17/01/19 16:37:40 INFO hive.metastore.HiveMetaStore: Added admin role in metastore\n17/01/19 16:37:40 INFO hive.metastore.HiveMetaStore: Added public role in metastore\n17/01/19 16:37:40 INFO hive.metastore.HiveMetaStore: No user is added in admin role, since config is empty\n17/01/19 16:37:40 INFO hive.metastore.HiveMetaStore: 0: get_all_databases\n17/01/19 16:37:40 INFO metastore.HiveMetaStore.audit: ugi=sc07-a3c399a7caae2d-99fc3133bdbb\tip=unknown-ip-addr\tcmd=get_all_databases\t\n17/01/19 16:37:41 INFO hive.metastore.HiveMetaStore: 0: get_functions: db=default pat=*\n17/01/19 16:37:41 INFO metastore.HiveMetaStore.audit: ugi=sc07-a3c399a7caae2d-99fc3133bdbb\tip=unknown-ip-addr\tcmd=get_functions: db=default pat=*\t\n17/01/19 16:37:41 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MResourceUri\" is tagged as \"embedded-only\" so does not have its own datastore table.\n17/01/19 16:37:41 INFO ql.session.SessionState: Created local directory: /gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/tmp/4a0603eb-569a-4c19-97c3-6c1ec7551327_resources\n17/01/19 16:37:41 INFO ql.session.SessionState: Created HDFS directory: /tmp/hive/sc07-a3c399a7caae2d-99fc3133bdbb/4a0603eb-569a-4c19-97c3-6c1ec7551327\n17/01/19 16:37:41 INFO ql.session.SessionState: Created local directory: /gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/tmp/sc07-a3c399a7caae2d-99fc3133bdbb/4a0603eb-569a-4c19-97c3-6c1ec7551327\n17/01/19 16:37:41 INFO ql.session.SessionState: Created HDFS directory: /tmp/hive/sc07-a3c399a7caae2d-99fc3133bdbb/4a0603eb-569a-4c19-97c3-6c1ec7551327/_tmp_space.db\n17/01/19 16:37:41 INFO sql.hive.HiveContext: default warehouse location is /user/hive/warehouse\n17/01/19 16:37:41 INFO sql.hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.\n17/01/19 16:37:41 INFO hive.client.ClientWrapper: Inspected Hadoop version: 2.6.0\n17/01/19 16:37:41 INFO hive.client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0\n17/01/19 16:37:41 INFO hive.metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore\n17/01/19 16:37:41 INFO hive.metastore.ObjectStore: ObjectStore, initialize called\n17/01/19 16:37:41 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n17/01/19 16:37:41 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored\n17/01/19 16:37:43 INFO hive.metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n17/01/19 16:37:44 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n17/01/19 16:37:44 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n17/01/19 16:37:45 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MFieldSchema\" is tagged as \"embedded-only\" so does not have its own datastore table.\n17/01/19 16:37:45 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MOrder\" is tagged as \"embedded-only\" so does not have its own datastore table.\n17/01/19 16:37:45 INFO hive.metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY\n17/01/19 16:37:45 INFO hive.metastore.ObjectStore: Initialized ObjectStore\n17/01/19 16:37:45 WARN hive.metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0\n17/01/19 16:37:45 WARN hive.metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException\n17/01/19 16:37:45 INFO hive.metastore.HiveMetaStore: Added admin role in metastore\n17/01/19 16:37:45 INFO hive.metastore.HiveMetaStore: Added public role in metastore\n17/01/19 16:37:45 INFO hive.metastore.HiveMetaStore: No user is added in admin role, since config is empty\n17/01/19 16:37:45 INFO hive.metastore.HiveMetaStore: 0: get_all_databases\n17/01/19 16:37:45 INFO metastore.HiveMetaStore.audit: ugi=sc07-a3c399a7caae2d-99fc3133bdbb\tip=unknown-ip-addr\tcmd=get_all_databases\t\n17/01/19 16:37:45 INFO hive.metastore.HiveMetaStore: 0: get_functions: db=default pat=*\n17/01/19 16:37:45 INFO metastore.HiveMetaStore.audit: ugi=sc07-a3c399a7caae2d-99fc3133bdbb\tip=unknown-ip-addr\tcmd=get_functions: db=default pat=*\t\n17/01/19 16:37:45 INFO DataNucleus.Datastore: The class \"org.apache.hadoop.hive.metastore.model.MResourceUri\" is tagged as \"embedded-only\" so does not have its own datastore table.\n17/01/19 16:37:46 INFO ql.session.SessionState: Created local directory: /gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/tmp/cf95ad20-e1de-4dbc-9118-03d3e04f1b01_resources\n17/01/19 16:37:46 INFO ql.session.SessionState: Created HDFS directory: /tmp/hive/sc07-a3c399a7caae2d-99fc3133bdbb/cf95ad20-e1de-4dbc-9118-03d3e04f1b01\n17/01/19 16:37:46 INFO ql.session.SessionState: Created local directory: /gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/notebook/tmp/sc07-a3c399a7caae2d-99fc3133bdbb/cf95ad20-e1de-4dbc-9118-03d3e04f1b01\n17/01/19 16:37:46 INFO ql.session.SessionState: Created HDFS directory: /tmp/hive/sc07-a3c399a7caae2d-99fc3133bdbb/cf95ad20-e1de-4dbc-9118-03d3e04f1b01/_tmp_space.db\nUse connectorVersion=1.6.4, dbName=ratingdb, indexName=null, viewName=null,jsonstore.rdd.partitions=5, jsonstore.rdd.maxInPartition=-1,jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=900000,bulkSize=20, schemaSampleSize=-1\n[WARN] [01/19/2017 16:37:46.595] [Thread-7] [JsonStoreDataAccess(akka://CloudantSpark-e7df7a6d-721f-48ea-ac30-16843ef7e629)] Loading data from Cloudant using query: https://875bb6df-554d-48d3-a8b2-f86cb2ce56cc-bluemix.cloudant.com/ratingdb/_all_docs?limit=1\n17/01/19 16:37:47 INFO spark.common.JsonStoreRDD: Partition config - total=5, limit=200000 for totalRows of 1000000\n17/01/19 16:37:47 INFO apache.spark.SparkContext: Starting job: json at DefaultSource.scala:130\n17/01/19 16:37:47 INFO spark.scheduler.DAGScheduler: Got job 0 (json at DefaultSource.scala:130) with 5 output partitions\n17/01/19 16:37:47 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 0 (json at DefaultSource.scala:130)\n17/01/19 16:37:47 INFO spark.scheduler.DAGScheduler: Parents of final stage: List()\n17/01/19 16:37:47 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:37:47 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at json at DefaultSource.scala:130), which has no missing parents\n17/01/19 16:37:47 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(0)\n17/01/19 16:37:47 INFO spark.storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KB, free 4.1 KB)\n17/01/19 16:37:48 INFO spark.storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.5 KB, free 6.6 KB)\n17/01/19 16:37:48 INFO spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.143.133.233:45916 (size: 2.5 KB, free: 909.0 MB)\n17/01/19 16:37:48 INFO apache.spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:37:48 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at DefaultSource.scala:130)\n17/01/19 16:37:48 INFO cluster.ego.EGODeployScheduler: Adding task set 0.0 with 5 tasks\n17/01/19 16:37:48 INFO cluster.ego.EGOFineGrainedSchedulerBackend: <EVENT> Spark driver SPARKDRIVER:b00c106a-9502-47d7-9b7e-b3f185c05375 workload coming in\n17/01/19 16:37:53 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (yp-spark-dal09-env5-0033:53120) with ID cbdf8a13-f34a-4ac1-b2fd-0bd6e9c731d9\n17/01/19 16:37:53 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2744 bytes)\n17/01/19 16:37:53 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager yp-spark-dal09-env5-0033:38602 with 4.3 GB RAM, BlockManagerId(cbdf8a13-f34a-4ac1-b2fd-0bd6e9c731d9, yp-spark-dal09-env5-0033, 38602)\n17/01/19 16:37:53 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2744 bytes)\n17/01/19 16:37:53 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, yp-spark-dal09-env5-0033, partition 2,PROCESS_LOCAL, 2744 bytes)\n17/01/19 16:37:53 INFO cluster.ego.EGOFineGrainedSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (yp-spark-dal09-env5-0024:35270) with ID 0def255c-82f7-49e1-bc16-f847bd27bffd\n17/01/19 16:37:53 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, yp-spark-dal09-env5-0024, partition 3,PROCESS_LOCAL, 2744 bytes)\n17/01/19 16:37:53 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, yp-spark-dal09-env5-0024, partition 4,PROCESS_LOCAL, 2744 bytes)\n17/01/19 16:37:53 INFO spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 2.5 KB, free: 4.3 GB)\n17/01/19 16:37:53 INFO spark.storage.BlockManagerMasterEndpoint: Registering block manager yp-spark-dal09-env5-0024:38984 with 4.3 GB RAM, BlockManagerId(0def255c-82f7-49e1-bc16-f847bd27bffd, yp-spark-dal09-env5-0024, 38984)\n17/01/19 16:37:54 INFO spark.storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 2.5 KB, free: 4.3 GB)\n17/01/19 16:38:12 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 18453 ms on yp-spark-dal09-env5-0033 (1/5)\n17/01/19 16:38:12 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 18725 ms on yp-spark-dal09-env5-0033 (2/5)\n17/01/19 16:38:15 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 22117 ms on yp-spark-dal09-env5-0033 (3/5)\n17/01/19 16:38:27 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 33147 ms on yp-spark-dal09-env5-0024 (4/5)\n17/01/19 16:38:27 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 33296 ms on yp-spark-dal09-env5-0024 (5/5)\n17/01/19 16:38:27 INFO spark.scheduler.DAGScheduler: ResultStage 0 (json at DefaultSource.scala:130) finished in 39.248 s\n17/01/19 16:38:27 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n17/01/19 16:38:27 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(0)\n17/01/19 16:38:27 INFO spark.scheduler.DAGScheduler: Job 0 finished: json at DefaultSource.scala:130, took 39.395089 s\n17/01/19 16:38:27 INFO spark.storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 220.7 KB, free 227.3 KB)\n17/01/19 16:38:27 INFO spark.storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 246.6 KB)\n17/01/19 16:38:27 INFO spark.storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.143.133.233:45916 (size: 19.3 KB, free: 909.0 MB)\n17/01/19 16:38:27 INFO apache.spark.SparkContext: Created broadcast 1 from rdd at DefaultSource.scala:54\n17/01/19 16:38:27 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-19 16:38:27 CST]\n17/01/19 16:38:27 INFO apache.spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:-2\n17/01/19 16:38:27 INFO spark.scheduler.DAGScheduler: Registering RDD 11 (count at NativeMethodAccessorImpl.java:-2)\n17/01/19 16:38:27 INFO spark.scheduler.DAGScheduler: Got job 1 (count at NativeMethodAccessorImpl.java:-2) with 1 output partitions\n17/01/19 16:38:27 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 2 (count at NativeMethodAccessorImpl.java:-2)\n17/01/19 16:38:27 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n17/01/19 16:38:27 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n17/01/19 16:38:27 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[11] at count at NativeMethodAccessorImpl.java:-2), which has no missing parents\n17/01/19 16:38:27 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(1)\n17/01/19 16:38:27 INFO spark.storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.9 KB, free 261.5 KB)\n17/01/19 16:38:27 INFO spark.storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.1 KB, free 268.6 KB)\n17/01/19 16:38:27 INFO spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.143.133.233:45916 (size: 7.1 KB, free: 909.0 MB)\n17/01/19 16:38:27 INFO apache.spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:38:27 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[11] at count at NativeMethodAccessorImpl.java:-2)\n17/01/19 16:38:27 INFO cluster.ego.EGODeployScheduler: Adding task set 1.0 with 5 tasks\n17/01/19 16:38:27 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 5, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2733 bytes)\n17/01/19 16:38:27 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 6, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2733 bytes)\n17/01/19 16:38:27 INFO spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 7.1 KB, free: 4.3 GB)\n17/01/19 16:38:27 INFO spark.storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.143.133.233:45916 in memory (size: 19.3 KB, free: 909.0 MB)\n17/01/19 16:38:27 INFO spark.storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 7.1 KB, free: 4.3 GB)\n17/01/19 16:38:27 INFO spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.143.133.233:45916 in memory (size: 2.5 KB, free: 909.0 MB)\n17/01/19 16:38:27 INFO spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 2.5 KB, free: 4.3 GB)\n17/01/19 16:38:27 INFO spark.storage.BlockManagerInfo: Removed broadcast_0_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 2.5 KB, free: 4.3 GB)\n17/01/19 16:38:27 INFO apache.spark.ContextCleaner: Cleaned accumulator 2\n17/01/19 16:38:28 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 7, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2733 bytes)\n17/01/19 16:38:29 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 8, yp-spark-dal09-env5-0033, partition 3,PROCESS_LOCAL, 2733 bytes)\n17/01/19 16:38:29 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 9, yp-spark-dal09-env5-0033, partition 4,PROCESS_LOCAL, 2733 bytes)\n17/01/19 16:38:41 INFO spark.storage.BlockManagerInfo: Added rdd_8_0 in memory on yp-spark-dal09-env5-0024:38984 (size: 11.9 MB, free: 4.3 GB)\n17/01/19 16:38:41 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 5) in 13584 ms on yp-spark-dal09-env5-0024 (1/5)\n17/01/19 16:38:45 INFO spark.storage.BlockManagerInfo: Added rdd_8_1 in memory on yp-spark-dal09-env5-0033:38602 (size: 12.3 MB, free: 4.3 GB)\n17/01/19 16:38:45 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 6) in 17711 ms on yp-spark-dal09-env5-0033 (2/5)\n17/01/19 16:38:49 INFO spark.storage.BlockManagerInfo: Added rdd_8_2 in memory on yp-spark-dal09-env5-0024:38984 (size: 12.2 MB, free: 4.3 GB)\n17/01/19 16:38:49 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 7) in 21197 ms on yp-spark-dal09-env5-0024 (3/5)\n17/01/19 16:38:55 INFO spark.storage.BlockManagerInfo: Added rdd_8_3 in memory on yp-spark-dal09-env5-0033:38602 (size: 12.4 MB, free: 4.3 GB)\n17/01/19 16:38:55 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 8) in 26655 ms on yp-spark-dal09-env5-0033 (4/5)\n17/01/19 16:39:00 INFO spark.storage.BlockManagerInfo: Added rdd_8_4 in memory on yp-spark-dal09-env5-0033:38602 (size: 12.3 MB, free: 4.2 GB)\n17/01/19 16:39:00 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 9) in 31150 ms on yp-spark-dal09-env5-0033 (5/5)\n17/01/19 16:39:00 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:-2) finished in 32.438 s\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:00 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(1)\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 2)\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at count at NativeMethodAccessorImpl.java:-2), which has no missing parents\n17/01/19 16:39:00 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(2)\n17/01/19 16:39:00 INFO spark.storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.3 KB, free 31.2 KB)\n17/01/19 16:39:00 INFO spark.storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.6 KB, free 35.8 KB)\n17/01/19 16:39:00 INFO spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.143.133.233:45916 (size: 4.6 KB, free: 909.0 MB)\n17/01/19 16:39:00 INFO apache.spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at count at NativeMethodAccessorImpl.java:-2)\n17/01/19 16:39:00 INFO cluster.ego.EGODeployScheduler: Adding task set 2.0 with 1 tasks\n17/01/19 16:39:00 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 10, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1999 bytes)\n17/01/19 16:39:00 INFO spark.storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 4.6 KB, free: 4.3 GB)\n17/01/19 16:39:00 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:00 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 241 bytes\n17/01/19 16:39:00 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 10) in 137 ms on yp-spark-dal09-env5-0024 (1/1)\n17/01/19 16:39:00 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: ResultStage 2 (count at NativeMethodAccessorImpl.java:-2) finished in 0.137 s\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: Job 1 finished: count at NativeMethodAccessorImpl.java:-2, took 32.628724 s\n17/01/19 16:39:00 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(2)\n17/01/19 16:39:00 INFO CloudantRecommender: [Found, 1000000, records in Cloudant]\n17/01/19 16:39:00 INFO CloudantRecommender: [Starting train model: , 2017-01-19 16:39:00 CST]\n17/01/19 16:39:00 INFO apache.spark.SparkContext: Starting job: runJob at PythonRDD.scala:393\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: Got job 2 (runJob at PythonRDD.scala:393) with 1 output partitions\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:393)\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: Parents of final stage: List()\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[18] at RDD at PythonRDD.scala:43), which has no missing parents\n17/01/19 16:39:00 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(3)\n17/01/19 16:39:00 INFO spark.storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.4 KB, free 49.2 KB)\n17/01/19 16:39:00 INFO spark.storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.1 KB, free 56.3 KB)\n17/01/19 16:39:00 INFO spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.143.133.233:45916 (size: 7.1 KB, free: 909.0 MB)\n17/01/19 16:39:00 INFO apache.spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:00 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[18] at RDD at PythonRDD.scala:43)\n17/01/19 16:39:00 INFO cluster.ego.EGODeployScheduler: Adding task set 3.0 with 1 tasks\n17/01/19 16:39:00 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 11, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2744 bytes)\n17/01/19 16:39:00 INFO spark.storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 7.1 KB, free: 4.3 GB)\n17/01/19 16:39:01 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 11) in 972 ms on yp-spark-dal09-env5-0024 (1/1)\n17/01/19 16:39:01 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \n17/01/19 16:39:01 INFO spark.scheduler.DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:393) finished in 0.974 s\n17/01/19 16:39:01 INFO spark.scheduler.DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:393, took 0.988924 s\n17/01/19 16:39:01 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(3)\n17/01/19 16:39:01 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:596\n17/01/19 16:39:01 INFO spark.scheduler.DAGScheduler: Registering RDD 22 (mapPartitions at ALS.scala:837)\n17/01/19 16:39:01 INFO spark.scheduler.DAGScheduler: Registering RDD 25 (map at ALS.scala:1080)\n17/01/19 16:39:01 INFO spark.scheduler.DAGScheduler: Got job 3 (count at ALS.scala:596) with 2 output partitions\n17/01/19 16:39:01 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 6 (count at ALS.scala:596)\n17/01/19 16:39:01 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n17/01/19 16:39:01 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 5)\n17/01/19 16:39:01 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[22] at mapPartitions at ALS.scala:837), which has no missing parents\n17/01/19 16:39:01 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(4)\n17/01/19 16:39:01 INFO spark.storage.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.4 KB, free 71.6 KB)\n17/01/19 16:39:01 INFO spark.storage.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.8 KB, free 79.5 KB)\n17/01/19 16:39:01 INFO spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.143.133.233:45916 (size: 7.8 KB, free: 909.0 MB)\n17/01/19 16:39:01 INFO apache.spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:01 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[22] at mapPartitions at ALS.scala:837)\n17/01/19 16:39:01 INFO cluster.ego.EGODeployScheduler: Adding task set 4.0 with 5 tasks\n17/01/19 16:39:01 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 12, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2733 bytes)\n17/01/19 16:39:01 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 13, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2733 bytes)\n17/01/19 16:39:01 INFO spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 7.8 KB, free: 4.3 GB)\n17/01/19 16:39:01 INFO spark.storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 7.8 KB, free: 4.2 GB)\n17/01/19 16:39:03 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 4.0 (TID 14, yp-spark-dal09-env5-0033, partition 3,PROCESS_LOCAL, 2733 bytes)\n17/01/19 16:39:03 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 4.0 (TID 15, yp-spark-dal09-env5-0033, partition 4,PROCESS_LOCAL, 2733 bytes)\n17/01/19 16:39:04 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 16, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 2733 bytes)\n17/01/19 16:39:04 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 13) in 2943 ms on yp-spark-dal09-env5-0024 (1/5)\n17/01/19 16:39:05 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 12) in 3603 ms on yp-spark-dal09-env5-0033 (2/5)\n17/01/19 16:39:05 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 4.0 (TID 15) in 2695 ms on yp-spark-dal09-env5-0033 (3/5)\n17/01/19 16:39:05 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 4.0 (TID 14) in 2723 ms on yp-spark-dal09-env5-0033 (4/5)\n17/01/19 16:39:06 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 16) in 2872 ms on yp-spark-dal09-env5-0024 (5/5)\n17/01/19 16:39:06 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n17/01/19 16:39:06 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 4 (mapPartitions at ALS.scala:837) finished in 5.375 s\n17/01/19 16:39:06 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:06 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:06 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 5, ResultStage 6)\n17/01/19 16:39:06 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:06 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(4)\n17/01/19 16:39:06 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[25] at map at ALS.scala:1080), which has no missing parents\n17/01/19 16:39:06 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(5)\n17/01/19 16:39:06 INFO spark.storage.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 16.5 KB, free 96.0 KB)\n17/01/19 16:39:06 INFO spark.storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.3 KB, free 104.3 KB)\n17/01/19 16:39:06 INFO spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.143.133.233:45916 (size: 8.3 KB, free: 909.0 MB)\n17/01/19 16:39:06 INFO apache.spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:06 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[25] at map at ALS.scala:1080)\n17/01/19 16:39:06 INFO cluster.ego.EGODeployScheduler: Adding task set 5.0 with 5 tasks\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 17, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1883 bytes)\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 18, yp-spark-dal09-env5-0033, partition 1,NODE_LOCAL, 1883 bytes)\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 19, yp-spark-dal09-env5-0024, partition 2,NODE_LOCAL, 1883 bytes)\n17/01/19 16:39:07 INFO spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 8.3 KB, free: 4.2 GB)\n17/01/19 16:39:07 INFO spark.storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 8.3 KB, free: 4.3 GB)\n17/01/19 16:39:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:07 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 253 bytes\n17/01/19 16:39:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:07 INFO spark.storage.BlockManagerInfo: Added rdd_24_0 in memory on yp-spark-dal09-env5-0024:38984 (size: 16.0 B, free: 4.3 GB)\n17/01/19 16:39:07 INFO spark.storage.BlockManagerInfo: Added rdd_24_2 in memory on yp-spark-dal09-env5-0024:38984 (size: 16.0 B, free: 4.3 GB)\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 5.0 (TID 20, yp-spark-dal09-env5-0024, partition 3,NODE_LOCAL, 1883 bytes)\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 17) in 60 ms on yp-spark-dal09-env5-0024 (1/5)\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 5.0 (TID 21, yp-spark-dal09-env5-0024, partition 4,NODE_LOCAL, 1883 bytes)\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 19) in 60 ms on yp-spark-dal09-env5-0024 (2/5)\n17/01/19 16:39:07 INFO spark.storage.BlockManagerInfo: Added rdd_24_4 in memory on yp-spark-dal09-env5-0024:38984 (size: 2.8 MB, free: 4.3 GB)\n17/01/19 16:39:07 INFO spark.storage.BlockManagerInfo: Added rdd_24_1 in memory on yp-spark-dal09-env5-0033:38602 (size: 3.0 MB, free: 4.2 GB)\n17/01/19 16:39:07 INFO spark.storage.BlockManagerInfo: Added rdd_24_3 in memory on yp-spark-dal09-env5-0024:38984 (size: 5.7 MB, free: 4.2 GB)\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 5.0 (TID 21) in 460 ms on yp-spark-dal09-env5-0024 (3/5)\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 18) in 546 ms on yp-spark-dal09-env5-0033 (4/5)\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 5.0 (TID 20) in 537 ms on yp-spark-dal09-env5-0024 (5/5)\n17/01/19 16:39:07 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n17/01/19 16:39:07 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 5 (map at ALS.scala:1080) finished in 0.596 s\n17/01/19 16:39:07 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:07 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:07 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 6)\n17/01/19 16:39:07 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:07 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(5)\n17/01/19 16:39:07 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 6 (userOutBlocks MapPartitionsRDD[28] at mapValues at ALS.scala:1117), which has no missing parents\n17/01/19 16:39:07 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(6)\n17/01/19 16:39:07 INFO spark.storage.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 17.1 KB, free 121.4 KB)\n17/01/19 16:39:07 INFO spark.storage.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.5 KB, free 129.9 KB)\n17/01/19 16:39:07 INFO spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.143.133.233:45916 (size: 8.5 KB, free: 909.0 MB)\n17/01/19 16:39:07 INFO apache.spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:07 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (userOutBlocks MapPartitionsRDD[28] at mapValues at ALS.scala:1117)\n17/01/19 16:39:07 INFO cluster.ego.EGODeployScheduler: Adding task set 6.0 with 2 tasks\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 22, yp-spark-dal09-env5-0033, partition 1,NODE_LOCAL, 1894 bytes)\n17/01/19 16:39:07 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 23, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1894 bytes)\n17/01/19 16:39:07 INFO spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:07 INFO spark.storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:07 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 253 bytes\n17/01/19 16:39:07 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added rdd_27_0 in memory on yp-spark-dal09-env5-0024:38984 (size: 3.8 MB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added rdd_27_1 in memory on yp-spark-dal09-env5-0033:38602 (size: 3.9 MB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added rdd_28_0 in memory on yp-spark-dal09-env5-0024:38984 (size: 23.7 KB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 23) in 468 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added rdd_28_1 in memory on yp-spark-dal09-env5-0033:38602 (size: 23.7 KB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 22) in 472 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:08 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: ResultStage 6 (count at ALS.scala:596) finished in 0.473 s\n17/01/19 16:39:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(6)\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Job 3 finished: count at ALS.scala:596, took 6.484741 s\n17/01/19 16:39:08 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:604\n17/01/19 16:39:08 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 253 bytes\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Registering RDD 30 (map at ALS.scala:1080)\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Got job 4 (count at ALS.scala:604) with 2 output partitions\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 9 (count at ALS.scala:604)\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[30] at map at ALS.scala:1080), which has no missing parents\n17/01/19 16:39:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(8)\n17/01/19 16:39:08 INFO spark.storage.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.7 KB, free 146.6 KB)\n17/01/19 16:39:08 INFO spark.storage.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.3 KB, free 154.9 KB)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.143.133.233:45916 (size: 8.3 KB, free: 908.9 MB)\n17/01/19 16:39:08 INFO apache.spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[30] at map at ALS.scala:1080)\n17/01/19 16:39:08 INFO cluster.ego.EGODeployScheduler: Adding task set 8.0 with 5 tasks\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 24, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 1883 bytes)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 25, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 1883 bytes)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 26, yp-spark-dal09-env5-0024, partition 2,PROCESS_LOCAL, 1883 bytes)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 8.3 KB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 8.3 KB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 8.0 (TID 27, yp-spark-dal09-env5-0024, partition 3,PROCESS_LOCAL, 1883 bytes)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 8.0 (TID 26) in 18 ms on yp-spark-dal09-env5-0024 (1/5)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Starting task 4.0 in stage 8.0 (TID 28, yp-spark-dal09-env5-0024, partition 4,PROCESS_LOCAL, 1883 bytes)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 24) in 19 ms on yp-spark-dal09-env5-0024 (2/5)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 25) in 98 ms on yp-spark-dal09-env5-0033 (3/5)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Finished task 4.0 in stage 8.0 (TID 28) in 94 ms on yp-spark-dal09-env5-0024 (4/5)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 8.0 (TID 27) in 169 ms on yp-spark-dal09-env5-0024 (5/5)\n17/01/19 16:39:08 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 8 (map at ALS.scala:1080) finished in 0.187 s\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 9)\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(8)\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 9 (itemOutBlocks MapPartitionsRDD[33] at mapValues at ALS.scala:1117), which has no missing parents\n17/01/19 16:39:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(9)\n17/01/19 16:39:08 INFO spark.storage.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 17.3 KB, free 172.2 KB)\n17/01/19 16:39:08 INFO spark.storage.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.5 KB, free 180.7 KB)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.143.133.233:45916 (size: 8.5 KB, free: 908.9 MB)\n17/01/19 16:39:08 INFO apache.spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (itemOutBlocks MapPartitionsRDD[33] at mapValues at ALS.scala:1117)\n17/01/19 16:39:08 INFO cluster.ego.EGODeployScheduler: Adding task set 9.0 with 2 tasks\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 29, yp-spark-dal09-env5-0033, partition 0,NODE_LOCAL, 1894 bytes)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 30, yp-spark-dal09-env5-0024, partition 1,NODE_LOCAL, 1894 bytes)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:08 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 253 bytes\n17/01/19 16:39:08 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added rdd_32_1 in memory on yp-spark-dal09-env5-0024:38984 (size: 3.7 MB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added rdd_33_1 in memory on yp-spark-dal09-env5-0024:38984 (size: 14.3 KB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 30) in 586 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added rdd_32_0 in memory on yp-spark-dal09-env5-0033:38602 (size: 3.9 MB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.storage.BlockManagerInfo: Added rdd_33_0 in memory on yp-spark-dal09-env5-0033:38602 (size: 14.2 KB, free: 4.2 GB)\n17/01/19 16:39:08 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 29) in 594 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:08 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: ResultStage 9 (count at ALS.scala:604) finished in 0.594 s\n17/01/19 16:39:08 INFO spark.scheduler.DAGScheduler: Job 4 finished: count at ALS.scala:604, took 0.836583 s\n17/01/19 16:39:08 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(9)\n17/01/19 16:39:08 INFO apache.spark.ContextCleaner: Cleaned accumulator 13\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned shuffle 0\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 12\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 11\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 10\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 9\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 8\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 7\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 6\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 5\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.143.133.233:45916 in memory (size: 8.5 KB, free: 908.9 MB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_9_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_9_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 20\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.143.133.233:45916 in memory (size: 8.3 KB, free: 909.0 MB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_8_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 8.3 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_8_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 8.3 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 19\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.143.133.233:45916 in memory (size: 8.5 KB, free: 909.0 MB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_7_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_7_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 18\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.143.133.233:45916 in memory (size: 8.3 KB, free: 909.0 MB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_6_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 8.3 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_6_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 8.3 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 17\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.143.133.233:45916 in memory (size: 7.8 KB, free: 909.0 MB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_5_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 7.8 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_5_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 7.8 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 16\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.143.133.233:45916 in memory (size: 7.1 KB, free: 909.0 MB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_4_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 7.1 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 15\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.143.133.233:45916 in memory (size: 4.6 KB, free: 909.0 MB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_3_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 4.6 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO apache.spark.ContextCleaner: Cleaned accumulator 14\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.143.133.233:45916 in memory (size: 7.1 KB, free: 909.0 MB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_2_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 7.1 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Removed broadcast_2_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 7.1 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:263\n17/01/19 16:39:09 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 253 bytes\n17/01/19 16:39:09 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 253 bytes\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 34 (map at ALS.scala:752)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 39 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 48 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 57 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 66 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 75 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 84 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 93 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 102 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 111 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 120 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 129 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 138 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 147 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 156 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 165 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 174 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 183 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 192 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 201 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 210 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 219 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 228 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 237 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 246 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 255 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 264 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 273 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 282 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 291 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 300 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 309 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 318 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 327 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 336 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 345 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 354 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 363 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 372 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 381 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Registering RDD 390 (flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Got job 5 (count at ALS.scala:263) with 2 output partitions\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 55 (count at ALS.scala:263)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13, ShuffleMapStage 54)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 54)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[34] at map at ALS.scala:752), which has no missing parents\n17/01/19 16:39:09 INFO spark.storage.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 17.1 KB, free 17.1 KB)\n17/01/19 16:39:09 INFO spark.storage.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.5 KB, free 25.6 KB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.143.133.233:45916 (size: 8.5 KB, free: 909.0 MB)\n17/01/19 16:39:09 INFO apache.spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[34] at map at ALS.scala:752)\n17/01/19 16:39:09 INFO cluster.ego.EGODeployScheduler: Adding task set 14.0 with 2 tasks\n17/01/19 16:39:09 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 31, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 1883 bytes)\n17/01/19 16:39:09 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 32, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 1883 bytes)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(14)\n17/01/19 16:39:09 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 31) in 104 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:09 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 32) in 109 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:09 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 14 (map at ALS.scala:752) finished in 0.111 s\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 15, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 16, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 17, ShuffleMapStage 39, ShuffleMapStage 18, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(14)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[39] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(15)\n17/01/19 16:39:09 INFO spark.storage.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 18.2 KB, free 43.8 KB)\n17/01/19 16:39:09 INFO spark.storage.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.8 KB, free 52.6 KB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.143.133.233:45916 (size: 8.8 KB, free: 909.0 MB)\n17/01/19 16:39:09 INFO apache.spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[39] at flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO cluster.ego.EGODeployScheduler: Adding task set 15.0 with 2 tasks\n17/01/19 16:39:09 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 33, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2110 bytes)\n17/01/19 16:39:09 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 34, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2110 bytes)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 8.8 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 8.8 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 44 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:09 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 44 is 239 bytes\n17/01/19 16:39:09 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 44 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:09 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 33) in 57 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:09 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 34) in 58 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:09 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 15 (flatMap at ALS.scala:1170) finished in 0.063 s\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 16, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 17, ShuffleMapStage 39, ShuffleMapStage 18, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(15)\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[48] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:09 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(16)\n17/01/19 16:39:09 INFO spark.storage.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 20.4 KB, free 72.9 KB)\n17/01/19 16:39:09 INFO spark.storage.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 9.5 KB, free 82.4 KB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.143.133.233:45916 (size: 9.5 KB, free: 909.0 MB)\n17/01/19 16:39:09 INFO apache.spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:09 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[48] at flatMap at ALS.scala:1170)\n17/01/19 16:39:09 INFO cluster.ego.EGODeployScheduler: Adding task set 16.0 with 2 tasks\n17/01/19 16:39:09 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 35, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:09 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 36, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 9.5 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO spark.storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 9.5 KB, free: 4.2 GB)\n17/01/19 16:39:09 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 43 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:09 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 43 is 236 bytes\n17/01/19 16:39:09 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 43 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:11 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 36) in 1525 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:11 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 35) in 1563 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:11 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \n17/01/19 16:39:11 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 16 (flatMap at ALS.scala:1170) finished in 1.563 s\n17/01/19 16:39:11 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:11 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:11 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 17, ShuffleMapStage 39, ShuffleMapStage 18, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:11 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:11 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(16)\n17/01/19 16:39:11 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[57] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:11 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(17)\n17/01/19 16:39:11 INFO spark.storage.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 21.3 KB, free 103.7 KB)\n17/01/19 16:39:11 INFO spark.storage.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.7 KB, free 113.4 KB)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.143.133.233:45916 (size: 9.7 KB, free: 909.0 MB)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.143.133.233:45916 in memory (size: 9.5 KB, free: 909.0 MB)\n17/01/19 16:39:11 INFO apache.spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:11 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[57] at flatMap at ALS.scala:1170)\n17/01/19 16:39:11 INFO cluster.ego.EGODeployScheduler: Adding task set 17.0 with 2 tasks\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Removed broadcast_12_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 9.5 KB, free: 4.2 GB)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Removed broadcast_12_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 9.5 KB, free: 4.2 GB)\n17/01/19 16:39:11 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 37, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:11 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 38, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.143.133.233:45916 in memory (size: 8.8 KB, free: 909.0 MB)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Removed broadcast_11_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 8.8 KB, free: 4.2 GB)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Removed broadcast_11_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 8.8 KB, free: 4.2 GB)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.143.133.233:45916 in memory (size: 8.5 KB, free: 909.0 MB)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Removed broadcast_10_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Removed broadcast_10_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 8.5 KB, free: 4.2 GB)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 9.7 KB, free: 4.2 GB)\n17/01/19 16:39:11 INFO spark.storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 9.7 KB, free: 4.2 GB)\n17/01/19 16:39:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 42 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:11 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 42 is 236 bytes\n17/01/19 16:39:11 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 42 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:12 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 38) in 962 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:12 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 37) in 996 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:12 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \n17/01/19 16:39:12 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 17 (flatMap at ALS.scala:1170) finished in 0.997 s\n17/01/19 16:39:12 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:12 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:12 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 18, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:12 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:12 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(17)\n17/01/19 16:39:12 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[66] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:12 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(18)\n17/01/19 16:39:12 INFO spark.storage.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 22.1 KB, free 53.1 KB)\n17/01/19 16:39:12 INFO spark.storage.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 9.9 KB, free 63.0 KB)\n17/01/19 16:39:12 INFO spark.storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.143.133.233:45916 (size: 9.9 KB, free: 909.0 MB)\n17/01/19 16:39:12 INFO apache.spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:12 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[66] at flatMap at ALS.scala:1170)\n17/01/19 16:39:12 INFO cluster.ego.EGODeployScheduler: Adding task set 18.0 with 2 tasks\n17/01/19 16:39:12 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 39, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:12 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 18.0 (TID 40, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:12 INFO spark.storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 9.9 KB, free: 4.2 GB)\n17/01/19 16:39:12 INFO spark.storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 9.9 KB, free: 4.2 GB)\n17/01/19 16:39:12 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 41 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:12 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 41 is 236 bytes\n17/01/19 16:39:12 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 41 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:13 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 18.0 (TID 40) in 903 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:13 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 39) in 921 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:13 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \n17/01/19 16:39:13 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 18 (flatMap at ALS.scala:1170) finished in 0.922 s\n17/01/19 16:39:13 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:13 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:13 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 19, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:13 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:13 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(18)\n17/01/19 16:39:13 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[75] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:13 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(19)\n17/01/19 16:39:13 INFO spark.storage.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 23.0 KB, free 86.0 KB)\n17/01/19 16:39:13 INFO spark.storage.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 10.1 KB, free 96.1 KB)\n17/01/19 16:39:13 INFO spark.storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.143.133.233:45916 (size: 10.1 KB, free: 909.0 MB)\n17/01/19 16:39:13 INFO apache.spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:13 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[75] at flatMap at ALS.scala:1170)\n17/01/19 16:39:13 INFO cluster.ego.EGODeployScheduler: Adding task set 19.0 with 2 tasks\n17/01/19 16:39:13 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 41, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:13 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 42, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:13 INFO spark.storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 10.1 KB, free: 4.2 GB)\n17/01/19 16:39:13 INFO spark.storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 10.1 KB, free: 4.2 GB)\n17/01/19 16:39:13 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 40 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:13 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 40 is 236 bytes\n17/01/19 16:39:13 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 40 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 41) in 961 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 42) in 973 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \n17/01/19 16:39:14 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 19 (flatMap at ALS.scala:1170) finished in 0.974 s\n17/01/19 16:39:14 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:14 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:14 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 20, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(19)\n17/01/19 16:39:14 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:14 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[84] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(20)\n17/01/19 16:39:14 INFO spark.storage.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 23.9 KB, free 120.0 KB)\n17/01/19 16:39:14 INFO spark.storage.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 10.3 KB, free 130.3 KB)\n17/01/19 16:39:14 INFO spark.storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.143.133.233:45916 (size: 10.3 KB, free: 909.0 MB)\n17/01/19 16:39:14 INFO apache.spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[84] at flatMap at ALS.scala:1170)\n17/01/19 16:39:14 INFO cluster.ego.EGODeployScheduler: Adding task set 20.0 with 2 tasks\n17/01/19 16:39:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 43, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 44, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:14 INFO spark.storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 10.3 KB, free: 4.2 GB)\n17/01/19 16:39:14 INFO spark.storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 10.3 KB, free: 4.2 GB)\n17/01/19 16:39:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 39 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 39 is 236 bytes\n17/01/19 16:39:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 39 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:15 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 20.0 (TID 43) in 905 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:15 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 44) in 931 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:15 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \n17/01/19 16:39:15 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 20 (flatMap at ALS.scala:1170) finished in 0.932 s\n17/01/19 16:39:15 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:15 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:15 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(20)\n17/01/19 16:39:15 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 21, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:15 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:15 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[93] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:15 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(21)\n17/01/19 16:39:15 INFO spark.storage.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 24.8 KB, free 155.0 KB)\n17/01/19 16:39:15 INFO spark.storage.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.4 KB, free 165.4 KB)\n17/01/19 16:39:15 INFO spark.storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.143.133.233:45916 (size: 10.4 KB, free: 909.0 MB)\n17/01/19 16:39:15 INFO apache.spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:15 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[93] at flatMap at ALS.scala:1170)\n17/01/19 16:39:15 INFO cluster.ego.EGODeployScheduler: Adding task set 21.0 with 2 tasks\n17/01/19 16:39:15 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 45, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:15 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 21.0 (TID 46, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:15 INFO spark.storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 10.4 KB, free: 4.2 GB)\n17/01/19 16:39:15 INFO spark.storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 10.4 KB, free: 4.2 GB)\n17/01/19 16:39:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:15 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 38 is 236 bytes\n17/01/19 16:39:15 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:16 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 21.0 (TID 46) in 963 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:16 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 45) in 1045 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:16 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \n17/01/19 16:39:16 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 21 (flatMap at ALS.scala:1170) finished in 1.047 s\n17/01/19 16:39:16 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:16 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:16 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 22, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:16 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:16 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(21)\n17/01/19 16:39:16 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[102] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:16 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(22)\n17/01/19 16:39:16 INFO spark.storage.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 25.6 KB, free 191.1 KB)\n17/01/19 16:39:16 INFO spark.storage.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 10.6 KB, free 201.7 KB)\n17/01/19 16:39:16 INFO spark.storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.143.133.233:45916 (size: 10.6 KB, free: 908.9 MB)\n17/01/19 16:39:16 INFO apache.spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:16 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[102] at flatMap at ALS.scala:1170)\n17/01/19 16:39:16 INFO cluster.ego.EGODeployScheduler: Adding task set 22.0 with 2 tasks\n17/01/19 16:39:16 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 47, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:16 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 22.0 (TID 48, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:16 INFO spark.storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 10.6 KB, free: 4.2 GB)\n17/01/19 16:39:16 INFO spark.storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 10.6 KB, free: 4.2 GB)\n17/01/19 16:39:16 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:16 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 37 is 236 bytes\n17/01/19 16:39:16 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:17 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 47) in 882 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:17 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 22.0 (TID 48) in 893 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:17 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \n17/01/19 16:39:17 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 22 (flatMap at ALS.scala:1170) finished in 0.893 s\n17/01/19 16:39:17 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:17 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:17 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 23, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:17 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:17 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(22)\n17/01/19 16:39:17 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[111] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:17 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(23)\n17/01/19 16:39:17 INFO spark.storage.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 26.5 KB, free 228.3 KB)\n17/01/19 16:39:17 INFO spark.storage.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 10.8 KB, free 239.1 KB)\n17/01/19 16:39:17 INFO spark.storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.143.133.233:45916 (size: 10.8 KB, free: 908.9 MB)\n17/01/19 16:39:17 INFO apache.spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:17 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[111] at flatMap at ALS.scala:1170)\n17/01/19 16:39:17 INFO cluster.ego.EGODeployScheduler: Adding task set 23.0 with 2 tasks\n17/01/19 16:39:17 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 23.0 (TID 49, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:17 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 50, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:17 INFO spark.storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 10.8 KB, free: 4.2 GB)\n17/01/19 16:39:17 INFO spark.storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 10.8 KB, free: 4.2 GB)\n17/01/19 16:39:17 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:17 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 36 is 236 bytes\n17/01/19 16:39:17 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:18 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 23.0 (TID 49) in 925 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:18 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 50) in 949 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:18 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \n17/01/19 16:39:18 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 23 (flatMap at ALS.scala:1170) finished in 0.950 s\n17/01/19 16:39:18 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:18 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:18 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 24, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:18 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:18 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(23)\n17/01/19 16:39:18 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[120] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:18 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(24)\n17/01/19 16:39:18 INFO spark.storage.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 27.4 KB, free 266.5 KB)\n17/01/19 16:39:18 INFO spark.storage.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 11.0 KB, free 277.4 KB)\n17/01/19 16:39:18 INFO spark.storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.143.133.233:45916 (size: 11.0 KB, free: 908.9 MB)\n17/01/19 16:39:18 INFO apache.spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:18 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[120] at flatMap at ALS.scala:1170)\n17/01/19 16:39:18 INFO cluster.ego.EGODeployScheduler: Adding task set 24.0 with 2 tasks\n17/01/19 16:39:18 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 24.0 (TID 51, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:18 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 52, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:18 INFO spark.storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 11.0 KB, free: 4.2 GB)\n17/01/19 16:39:18 INFO spark.storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 11.0 KB, free: 4.2 GB)\n17/01/19 16:39:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:18 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 35 is 236 bytes\n17/01/19 16:39:18 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:19 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 24.0 (TID 51) in 869 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:19 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 52) in 881 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:19 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \n17/01/19 16:39:19 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 24 (flatMap at ALS.scala:1170) finished in 0.882 s\n17/01/19 16:39:19 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:19 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:19 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(24)\n17/01/19 16:39:19 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ShuffleMapStage 25, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:19 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:19 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[129] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:19 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(25)\n17/01/19 16:39:19 INFO spark.storage.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 28.3 KB, free 305.7 KB)\n17/01/19 16:39:19 INFO spark.storage.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 11.1 KB, free 316.9 KB)\n17/01/19 16:39:19 INFO spark.storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.143.133.233:45916 (size: 11.1 KB, free: 908.9 MB)\n17/01/19 16:39:19 INFO apache.spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:19 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[129] at flatMap at ALS.scala:1170)\n17/01/19 16:39:19 INFO cluster.ego.EGODeployScheduler: Adding task set 25.0 with 2 tasks\n17/01/19 16:39:19 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 53, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:19 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 25.0 (TID 54, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:19 INFO spark.storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 11.1 KB, free: 4.2 GB)\n17/01/19 16:39:19 INFO spark.storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 11.1 KB, free: 4.2 GB)\n17/01/19 16:39:19 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:19 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 34 is 236 bytes\n17/01/19 16:39:19 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:20 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 25.0 (TID 54) in 925 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:20 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 53) in 948 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:20 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \n17/01/19 16:39:20 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 25 (flatMap at ALS.scala:1170) finished in 0.948 s\n17/01/19 16:39:20 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:20 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:20 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(25)\n17/01/19 16:39:20 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 26, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:20 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:20 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[138] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:20 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(26)\n17/01/19 16:39:20 INFO spark.storage.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 29.2 KB, free 346.0 KB)\n17/01/19 16:39:20 INFO spark.storage.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 11.4 KB, free 357.4 KB)\n17/01/19 16:39:20 INFO spark.storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.143.133.233:45916 (size: 11.4 KB, free: 908.9 MB)\n17/01/19 16:39:20 INFO apache.spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:20 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[138] at flatMap at ALS.scala:1170)\n17/01/19 16:39:20 INFO cluster.ego.EGODeployScheduler: Adding task set 26.0 with 2 tasks\n17/01/19 16:39:20 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 26.0 (TID 55, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:20 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 56, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:20 INFO spark.storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 11.4 KB, free: 4.2 GB)\n17/01/19 16:39:20 INFO spark.storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 11.4 KB, free: 4.2 GB)\n17/01/19 16:39:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:20 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:20 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 33 is 236 bytes\n17/01/19 16:39:20 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 33 is 236 bytes\n17/01/19 16:39:21 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 56) in 896 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:21 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 26.0 (TID 55) in 903 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:21 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \n17/01/19 16:39:21 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 26 (flatMap at ALS.scala:1170) finished in 0.904 s\n17/01/19 16:39:21 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:21 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:21 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 27, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:21 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(26)\n17/01/19 16:39:21 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[147] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:21 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(27)\n17/01/19 16:39:21 INFO spark.storage.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 30.0 KB, free 387.4 KB)\n17/01/19 16:39:21 INFO spark.storage.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 11.5 KB, free 398.9 KB)\n17/01/19 16:39:21 INFO spark.storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.143.133.233:45916 (size: 11.5 KB, free: 908.9 MB)\n17/01/19 16:39:21 INFO apache.spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:21 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[147] at flatMap at ALS.scala:1170)\n17/01/19 16:39:21 INFO cluster.ego.EGODeployScheduler: Adding task set 27.0 with 2 tasks\n17/01/19 16:39:21 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 27.0 (TID 57, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:21 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 58, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:21 INFO spark.storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 11.5 KB, free: 4.2 GB)\n17/01/19 16:39:21 INFO spark.storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 11.5 KB, free: 4.2 GB)\n17/01/19 16:39:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:21 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 32 is 236 bytes\n17/01/19 16:39:21 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:22 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 27.0 (TID 57) in 930 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:22 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 58) in 994 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:22 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \n17/01/19 16:39:22 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 27 (flatMap at ALS.scala:1170) finished in 0.995 s\n17/01/19 16:39:22 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:22 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:22 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 28, ShuffleMapStage 29)\n17/01/19 16:39:22 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:22 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(27)\n17/01/19 16:39:22 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[156] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:22 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(28)\n17/01/19 16:39:22 INFO spark.storage.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 30.9 KB, free 429.8 KB)\n17/01/19 16:39:22 INFO spark.storage.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 11.7 KB, free 441.5 KB)\n17/01/19 16:39:22 INFO spark.storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.143.133.233:45916 (size: 11.7 KB, free: 908.9 MB)\n17/01/19 16:39:22 INFO apache.spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:22 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[156] at flatMap at ALS.scala:1170)\n17/01/19 16:39:22 INFO cluster.ego.EGODeployScheduler: Adding task set 28.0 with 2 tasks\n17/01/19 16:39:22 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 28.0 (TID 59, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:22 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 60, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:22 INFO spark.storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 11.7 KB, free: 4.2 GB)\n17/01/19 16:39:22 INFO spark.storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 11.7 KB, free: 4.2 GB)\n17/01/19 16:39:22 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:22 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 31 is 236 bytes\n17/01/19 16:39:22 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:23 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 60) in 897 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:23 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 28.0 (TID 59) in 965 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:23 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \n17/01/19 16:39:23 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 28 (flatMap at ALS.scala:1170) finished in 0.965 s\n17/01/19 16:39:23 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:23 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:23 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(28)\n17/01/19 16:39:23 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 29)\n17/01/19 16:39:23 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:23 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[165] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:23 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(29)\n17/01/19 16:39:23 INFO spark.storage.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 31.8 KB, free 473.3 KB)\n17/01/19 16:39:23 INFO spark.storage.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 11.8 KB, free 485.1 KB)\n17/01/19 16:39:23 INFO spark.storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.143.133.233:45916 (size: 11.8 KB, free: 908.9 MB)\n17/01/19 16:39:23 INFO apache.spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:23 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[165] at flatMap at ALS.scala:1170)\n17/01/19 16:39:23 INFO cluster.ego.EGODeployScheduler: Adding task set 29.0 with 2 tasks\n17/01/19 16:39:23 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 29.0 (TID 61, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:23 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 62, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:23 INFO spark.storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 11.8 KB, free: 4.2 GB)\n17/01/19 16:39:23 INFO spark.storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 11.8 KB, free: 4.2 GB)\n17/01/19 16:39:23 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:23 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 30 is 236 bytes\n17/01/19 16:39:23 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 30 is 236 bytes\n17/01/19 16:39:23 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 29.0 (TID 61) in 923 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:24 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 62) in 935 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:24 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 29 (flatMap at ALS.scala:1170) finished in 0.935 s\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 30, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36)\n17/01/19 16:39:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(29)\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[174] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(30)\n17/01/19 16:39:24 INFO spark.storage.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.7 KB, free 517.8 KB)\n17/01/19 16:39:24 INFO spark.storage.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 12.2 KB, free 530.0 KB)\n17/01/19 16:39:24 INFO spark.storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.143.133.233:45916 (size: 12.2 KB, free: 908.9 MB)\n17/01/19 16:39:24 INFO apache.spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[174] at flatMap at ALS.scala:1170)\n17/01/19 16:39:24 INFO cluster.ego.EGODeployScheduler: Adding task set 30.0 with 2 tasks\n17/01/19 16:39:24 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 63, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:24 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 30.0 (TID 64, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:24 INFO spark.storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 12.2 KB, free: 4.2 GB)\n17/01/19 16:39:24 INFO spark.storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 12.2 KB, free: 4.2 GB)\n17/01/19 16:39:24 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:24 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 29 is 236 bytes\n17/01/19 16:39:24 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:24 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 30.0 (TID 64) in 872 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:24 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 63) in 893 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:24 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 30.0, whose tasks have all completed, from pool \n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 30 (flatMap at ALS.scala:1170) finished in 0.892 s\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 31, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36)\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(30)\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[183] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:24 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(31)\n17/01/19 16:39:24 INFO spark.storage.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 33.5 KB, free 563.5 KB)\n17/01/19 16:39:24 INFO spark.storage.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 12.4 KB, free 575.9 KB)\n17/01/19 16:39:24 INFO spark.storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.143.133.233:45916 (size: 12.4 KB, free: 908.8 MB)\n17/01/19 16:39:24 INFO apache.spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:24 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[183] at flatMap at ALS.scala:1170)\n17/01/19 16:39:24 INFO cluster.ego.EGODeployScheduler: Adding task set 31.0 with 2 tasks\n17/01/19 16:39:24 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 65, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:24 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 31.0 (TID 66, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:24 INFO spark.storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 12.4 KB, free: 4.2 GB)\n17/01/19 16:39:24 INFO spark.storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 12.4 KB, free: 4.2 GB)\n17/01/19 16:39:24 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:24 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 28 is 236 bytes\n17/01/19 16:39:24 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:25 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 65) in 935 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:25 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 31.0 (TID 66) in 1021 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:25 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \n17/01/19 16:39:25 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 31 (flatMap at ALS.scala:1170) finished in 1.023 s\n17/01/19 16:39:25 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:25 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:25 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 32, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36)\n17/01/19 16:39:25 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(31)\n17/01/19 16:39:25 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:25 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[192] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:25 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(32)\n17/01/19 16:39:25 INFO spark.storage.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 34.4 KB, free 610.4 KB)\n17/01/19 16:39:25 INFO spark.storage.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 12.5 KB, free 622.9 KB)\n17/01/19 16:39:25 INFO spark.storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.143.133.233:45916 (size: 12.5 KB, free: 908.8 MB)\n17/01/19 16:39:25 INFO apache.spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:25 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[192] at flatMap at ALS.scala:1170)\n17/01/19 16:39:25 INFO cluster.ego.EGODeployScheduler: Adding task set 32.0 with 2 tasks\n17/01/19 16:39:25 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 32.0 (TID 67, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:25 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 68, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:25 INFO spark.storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 12.5 KB, free: 4.2 GB)\n17/01/19 16:39:25 INFO spark.storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 12.5 KB, free: 4.2 GB)\n17/01/19 16:39:25 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:25 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 27 is 236 bytes\n17/01/19 16:39:25 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:26 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 32.0 (TID 67) in 879 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:26 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 68) in 941 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:26 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \n17/01/19 16:39:26 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 32 (flatMap at ALS.scala:1170) finished in 0.943 s\n17/01/19 16:39:26 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:26 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:26 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36)\n17/01/19 16:39:26 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:26 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(32)\n17/01/19 16:39:26 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[201] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:26 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(33)\n17/01/19 16:39:26 INFO spark.storage.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 35.3 KB, free 658.2 KB)\n17/01/19 16:39:26 INFO spark.storage.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 12.8 KB, free 670.9 KB)\n17/01/19 16:39:26 INFO spark.storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.143.133.233:45916 (size: 12.8 KB, free: 908.8 MB)\n17/01/19 16:39:26 INFO apache.spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:26 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[201] at flatMap at ALS.scala:1170)\n17/01/19 16:39:26 INFO cluster.ego.EGODeployScheduler: Adding task set 33.0 with 2 tasks\n17/01/19 16:39:26 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 69, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:26 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 33.0 (TID 70, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:26 INFO spark.storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 12.8 KB, free: 4.2 GB)\n17/01/19 16:39:26 INFO spark.storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 12.8 KB, free: 4.2 GB)\n17/01/19 16:39:26 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:26 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:26 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 26 is 236 bytes\n17/01/19 16:39:26 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 26 is 236 bytes\n17/01/19 16:39:27 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 33.0 (TID 70) in 913 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:27 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 69) in 941 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:27 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \n17/01/19 16:39:27 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 33 (flatMap at ALS.scala:1170) finished in 0.942 s\n17/01/19 16:39:27 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:27 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:27 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(33)\n17/01/19 16:39:27 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36)\n17/01/19 16:39:27 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:27 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[210] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:27 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(34)\n17/01/19 16:39:27 INFO spark.storage.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 36.2 KB, free 707.1 KB)\n17/01/19 16:39:27 INFO spark.storage.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 12.9 KB, free 720.1 KB)\n17/01/19 16:39:27 INFO spark.storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.143.133.233:45916 (size: 12.9 KB, free: 908.8 MB)\n17/01/19 16:39:27 INFO apache.spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:27 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[210] at flatMap at ALS.scala:1170)\n17/01/19 16:39:27 INFO cluster.ego.EGODeployScheduler: Adding task set 34.0 with 2 tasks\n17/01/19 16:39:27 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 34.0 (TID 71, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:27 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 72, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:27 INFO spark.storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 12.9 KB, free: 4.2 GB)\n17/01/19 16:39:27 INFO spark.storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 12.9 KB, free: 4.2 GB)\n17/01/19 16:39:27 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:27 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 25 is 236 bytes\n17/01/19 16:39:27 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:28 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 72) in 886 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:28 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 34.0 (TID 71) in 886 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:28 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool \n17/01/19 16:39:28 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 34 (flatMap at ALS.scala:1170) finished in 0.888 s\n17/01/19 16:39:28 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:28 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:28 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(34)\n17/01/19 16:39:28 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 35, ShuffleMapStage 36)\n17/01/19 16:39:28 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:28 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[219] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:28 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(35)\n17/01/19 16:39:28 INFO spark.storage.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 37.1 KB, free 757.1 KB)\n17/01/19 16:39:28 INFO spark.storage.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 13.1 KB, free 770.2 KB)\n17/01/19 16:39:28 INFO spark.storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.143.133.233:45916 (size: 13.1 KB, free: 908.8 MB)\n17/01/19 16:39:28 INFO apache.spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:28 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[219] at flatMap at ALS.scala:1170)\n17/01/19 16:39:28 INFO cluster.ego.EGODeployScheduler: Adding task set 35.0 with 2 tasks\n17/01/19 16:39:28 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 35.0 (TID 73, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:28 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 74, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:28 INFO spark.storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 13.1 KB, free: 4.2 GB)\n17/01/19 16:39:28 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:28 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 24 is 236 bytes\n17/01/19 16:39:28 INFO spark.storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 13.1 KB, free: 4.2 GB)\n17/01/19 16:39:28 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:29 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 35.0 (TID 73) in 921 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:29 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 74) in 940 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:29 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \n17/01/19 16:39:29 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 35 (flatMap at ALS.scala:1170) finished in 0.940 s\n17/01/19 16:39:29 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:29 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:29 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(35)\n17/01/19 16:39:29 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 36)\n17/01/19 16:39:29 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:29 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[228] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:29 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(36)\n17/01/19 16:39:29 INFO spark.storage.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 37.9 KB, free 808.2 KB)\n17/01/19 16:39:29 INFO spark.storage.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 13.2 KB, free 821.4 KB)\n17/01/19 16:39:29 INFO spark.storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.143.133.233:45916 (size: 13.2 KB, free: 908.8 MB)\n17/01/19 16:39:29 INFO apache.spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:29 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[228] at flatMap at ALS.scala:1170)\n17/01/19 16:39:29 INFO cluster.ego.EGODeployScheduler: Adding task set 36.0 with 2 tasks\n17/01/19 16:39:29 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 36.0 (TID 75, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:29 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 76, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:29 INFO spark.storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 13.2 KB, free: 4.2 GB)\n17/01/19 16:39:29 INFO spark.storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 13.2 KB, free: 4.2 GB)\n17/01/19 16:39:29 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:29 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 23 is 236 bytes\n17/01/19 16:39:29 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:30 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 36.0 (TID 75) in 879 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:30 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 76) in 892 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:30 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \n17/01/19 16:39:30 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 36 (flatMap at ALS.scala:1170) finished in 0.893 s\n17/01/19 16:39:30 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:30 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:30 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 37, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41)\n17/01/19 16:39:30 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:30 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(36)\n17/01/19 16:39:30 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[237] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:30 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(37)\n17/01/19 16:39:30 INFO spark.storage.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 38.8 KB, free 860.2 KB)\n17/01/19 16:39:30 INFO spark.storage.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 13.4 KB, free 873.6 KB)\n17/01/19 16:39:30 INFO spark.storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.143.133.233:45916 (size: 13.4 KB, free: 908.8 MB)\n17/01/19 16:39:30 INFO apache.spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:30 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[237] at flatMap at ALS.scala:1170)\n17/01/19 16:39:30 INFO cluster.ego.EGODeployScheduler: Adding task set 37.0 with 2 tasks\n17/01/19 16:39:30 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 77, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:30 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 37.0 (TID 78, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:30 INFO spark.storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 13.4 KB, free: 4.2 GB)\n17/01/19 16:39:30 INFO spark.storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 13.4 KB, free: 4.2 GB)\n17/01/19 16:39:30 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:30 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 22 is 236 bytes\n17/01/19 16:39:30 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:31 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 37.0 (TID 78) in 911 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:31 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 77) in 941 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:31 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \n17/01/19 16:39:31 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 37 (flatMap at ALS.scala:1170) finished in 0.942 s\n17/01/19 16:39:31 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:31 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:31 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41)\n17/01/19 16:39:31 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:31 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(37)\n17/01/19 16:39:31 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[246] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:31 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(38)\n17/01/19 16:39:31 INFO spark.storage.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 39.7 KB, free 913.3 KB)\n17/01/19 16:39:31 INFO spark.storage.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 13.6 KB, free 927.0 KB)\n17/01/19 16:39:31 INFO spark.storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.143.133.233:45916 (size: 13.6 KB, free: 908.7 MB)\n17/01/19 16:39:31 INFO apache.spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:31 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[246] at flatMap at ALS.scala:1170)\n17/01/19 16:39:31 INFO cluster.ego.EGODeployScheduler: Adding task set 38.0 with 2 tasks\n17/01/19 16:39:31 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 79, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:31 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 38.0 (TID 80, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:31 INFO spark.storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 13.6 KB, free: 4.2 GB)\n17/01/19 16:39:31 INFO spark.storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 13.6 KB, free: 4.2 GB)\n17/01/19 16:39:31 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:31 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 21 is 236 bytes\n17/01/19 16:39:31 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:32 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 38.0 (TID 80) in 871 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:32 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 79) in 884 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:32 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \n17/01/19 16:39:32 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 38 (flatMap at ALS.scala:1170) finished in 0.884 s\n17/01/19 16:39:32 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:32 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:32 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41)\n17/01/19 16:39:32 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:32 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(38)\n17/01/19 16:39:32 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 39 (MapPartitionsRDD[255] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:32 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(39)\n17/01/19 16:39:32 INFO spark.storage.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 40.6 KB, free 967.5 KB)\n17/01/19 16:39:32 INFO spark.storage.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 13.8 KB, free 981.4 KB)\n17/01/19 16:39:32 INFO spark.storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.143.133.233:45916 (size: 13.8 KB, free: 908.7 MB)\n17/01/19 16:39:32 INFO apache.spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:32 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 39 (MapPartitionsRDD[255] at flatMap at ALS.scala:1170)\n17/01/19 16:39:32 INFO cluster.ego.EGODeployScheduler: Adding task set 39.0 with 2 tasks\n17/01/19 16:39:32 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 39.0 (TID 81, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:32 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 82, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:32 INFO spark.storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 13.8 KB, free: 4.2 GB)\n17/01/19 16:39:32 INFO spark.storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 13.8 KB, free: 4.2 GB)\n17/01/19 16:39:32 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:32 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 20 is 236 bytes\n17/01/19 16:39:32 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:33 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 39.0 (TID 81) in 920 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:33 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 82) in 945 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:33 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 39.0, whose tasks have all completed, from pool \n17/01/19 16:39:33 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 39 (flatMap at ALS.scala:1170) finished in 0.946 s\n17/01/19 16:39:33 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:33 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:33 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46, ShuffleMapStage 40, ShuffleMapStage 41)\n17/01/19 16:39:33 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:33 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(39)\n17/01/19 16:39:33 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[264] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:33 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(40)\n17/01/19 16:39:33 INFO spark.storage.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 41.4 KB, free 1022.8 KB)\n17/01/19 16:39:33 INFO spark.storage.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 14.0 KB, free 1036.8 KB)\n17/01/19 16:39:33 INFO spark.storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.143.133.233:45916 (size: 14.0 KB, free: 908.7 MB)\n17/01/19 16:39:33 INFO apache.spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:33 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[264] at flatMap at ALS.scala:1170)\n17/01/19 16:39:33 INFO cluster.ego.EGODeployScheduler: Adding task set 40.0 with 2 tasks\n17/01/19 16:39:33 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 83, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:33 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 40.0 (TID 84, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:33 INFO spark.storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 14.0 KB, free: 4.2 GB)\n17/01/19 16:39:33 INFO spark.storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 14.0 KB, free: 4.2 GB)\n17/01/19 16:39:33 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:33 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 19 is 236 bytes\n17/01/19 16:39:33 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:34 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 40.0 (TID 84) in 876 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:34 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 83) in 890 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:34 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \n17/01/19 16:39:34 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 40 (flatMap at ALS.scala:1170) finished in 0.891 s\n17/01/19 16:39:34 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:34 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:34 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46, ShuffleMapStage 41)\n17/01/19 16:39:34 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:34 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(40)\n17/01/19 16:39:34 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[273] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:34 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(41)\n17/01/19 16:39:34 INFO spark.storage.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 42.3 KB, free 1079.1 KB)\n17/01/19 16:39:34 INFO spark.storage.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 14.2 KB, free 1093.3 KB)\n17/01/19 16:39:34 INFO spark.storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.143.133.233:45916 (size: 14.2 KB, free: 908.7 MB)\n17/01/19 16:39:34 INFO apache.spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:34 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[273] at flatMap at ALS.scala:1170)\n17/01/19 16:39:34 INFO cluster.ego.EGODeployScheduler: Adding task set 41.0 with 2 tasks\n17/01/19 16:39:34 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 41.0 (TID 85, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:34 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 86, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:34 INFO spark.storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 14.2 KB, free: 4.2 GB)\n17/01/19 16:39:34 INFO spark.storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 14.2 KB, free: 4.2 GB)\n17/01/19 16:39:34 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:34 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:34 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 18 is 236 bytes\n17/01/19 16:39:34 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 18 is 236 bytes\n17/01/19 16:39:35 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 41.0 (TID 85) in 921 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:35 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 86) in 935 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:35 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \n17/01/19 16:39:35 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 41 (flatMap at ALS.scala:1170) finished in 0.936 s\n17/01/19 16:39:35 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:35 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:35 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46)\n17/01/19 16:39:35 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:35 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(41)\n17/01/19 16:39:35 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[282] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:35 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(42)\n17/01/19 16:39:35 INFO spark.storage.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 43.2 KB, free 1136.5 KB)\n17/01/19 16:39:35 INFO spark.storage.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 14.3 KB, free 1150.8 KB)\n17/01/19 16:39:35 INFO spark.storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.143.133.233:45916 (size: 14.3 KB, free: 908.7 MB)\n17/01/19 16:39:35 INFO apache.spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:35 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[282] at flatMap at ALS.scala:1170)\n17/01/19 16:39:35 INFO cluster.ego.EGODeployScheduler: Adding task set 42.0 with 2 tasks\n17/01/19 16:39:35 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 42.0 (TID 87, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:35 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 88, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:35 INFO spark.storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 14.3 KB, free: 4.2 GB)\n17/01/19 16:39:35 INFO spark.storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 14.3 KB, free: 4.2 GB)\n17/01/19 16:39:35 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:35 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 17 is 236 bytes\n17/01/19 16:39:35 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:36 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 42.0 (TID 87) in 881 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:36 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 88) in 883 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:36 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \n17/01/19 16:39:36 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 42 (flatMap at ALS.scala:1170) finished in 0.885 s\n17/01/19 16:39:36 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:36 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:36 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46)\n17/01/19 16:39:36 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:36 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(42)\n17/01/19 16:39:36 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[291] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:36 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(43)\n17/01/19 16:39:36 INFO spark.storage.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 44.1 KB, free 1194.9 KB)\n17/01/19 16:39:36 INFO spark.storage.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 14.5 KB, free 1209.4 KB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.143.133.233:45916 (size: 14.5 KB, free: 908.7 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.143.133.233:45916 in memory (size: 14.3 KB, free: 908.7 MB)\n17/01/19 16:39:36 INFO apache.spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:36 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[291] at flatMap at ALS.scala:1170)\n17/01/19 16:39:36 INFO cluster.ego.EGODeployScheduler: Adding task set 43.0 with 2 tasks\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_38_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 14.3 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_38_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 14.3 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 43.0 (TID 89, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:36 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 43.0 (TID 90, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.143.133.233:45916 in memory (size: 14.2 KB, free: 908.7 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_37_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 14.2 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_37_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 14.2 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.143.133.233:45916 in memory (size: 14.0 KB, free: 908.7 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_36_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 14.0 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_36_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 14.0 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 14.5 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 14.5 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.143.133.233:45916 in memory (size: 13.8 KB, free: 908.7 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_35_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 13.8 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_35_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 13.8 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.143.133.233:45916 in memory (size: 13.6 KB, free: 908.7 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_34_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 13.6 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_34_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 13.6 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:36 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:36 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 16 is 236 bytes\n17/01/19 16:39:36 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 16 is 236 bytes\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.143.133.233:45916 in memory (size: 13.4 KB, free: 908.8 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_33_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 13.4 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_33_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 13.4 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.143.133.233:45916 in memory (size: 13.2 KB, free: 908.8 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_32_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 13.2 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_32_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 13.2 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.143.133.233:45916 in memory (size: 13.1 KB, free: 908.8 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_31_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 13.1 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_31_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 13.1 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.143.133.233:45916 in memory (size: 12.9 KB, free: 908.8 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_30_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 12.9 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_30_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 12.9 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.143.133.233:45916 in memory (size: 12.8 KB, free: 908.8 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_29_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 12.8 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_29_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 12.8 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.143.133.233:45916 in memory (size: 12.5 KB, free: 908.8 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_28_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 12.5 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_28_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 12.5 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.143.133.233:45916 in memory (size: 12.4 KB, free: 908.8 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_27_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 12.4 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_27_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 12.4 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.143.133.233:45916 in memory (size: 12.2 KB, free: 908.8 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_26_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 12.2 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_26_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 12.2 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.143.133.233:45916 in memory (size: 11.8 KB, free: 908.9 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_25_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 11.8 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_25_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 11.8 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.143.133.233:45916 in memory (size: 11.7 KB, free: 908.9 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_24_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 11.7 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_24_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 11.7 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.143.133.233:45916 in memory (size: 11.5 KB, free: 908.9 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_23_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 11.5 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_23_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 11.5 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.143.133.233:45916 in memory (size: 11.4 KB, free: 908.9 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_22_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 11.4 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_22_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 11.4 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.143.133.233:45916 in memory (size: 11.1 KB, free: 908.9 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_21_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 11.1 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_21_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 11.1 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.143.133.233:45916 in memory (size: 11.0 KB, free: 908.9 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_20_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 11.0 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_20_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 11.0 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.143.133.233:45916 in memory (size: 10.8 KB, free: 908.9 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_19_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 10.8 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_19_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 10.8 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.143.133.233:45916 in memory (size: 10.6 KB, free: 908.9 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_18_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 10.6 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_18_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 10.6 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.143.133.233:45916 in memory (size: 10.4 KB, free: 908.9 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_17_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 10.4 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_17_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 10.4 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.143.133.233:45916 in memory (size: 10.3 KB, free: 909.0 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_16_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 10.3 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_16_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 10.3 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.143.133.233:45916 in memory (size: 10.1 KB, free: 909.0 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_15_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 10.1 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_15_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 10.1 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.143.133.233:45916 in memory (size: 9.9 KB, free: 909.0 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_14_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 9.9 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_14_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 9.9 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.143.133.233:45916 in memory (size: 9.7 KB, free: 909.0 MB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_13_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 9.7 KB, free: 4.2 GB)\n17/01/19 16:39:36 INFO spark.storage.BlockManagerInfo: Removed broadcast_13_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 9.7 KB, free: 4.2 GB)\n17/01/19 16:39:37 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 43.0 (TID 90) in 915 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:37 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 43.0 (TID 89) in 949 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:37 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 43.0, whose tasks have all completed, from pool \n17/01/19 16:39:37 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 43 (flatMap at ALS.scala:1170) finished in 0.949 s\n17/01/19 16:39:37 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:37 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:37 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 46)\n17/01/19 16:39:37 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:37 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(43)\n17/01/19 16:39:37 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[300] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:37 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(44)\n17/01/19 16:39:37 INFO spark.storage.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 45.0 KB, free 103.5 KB)\n17/01/19 16:39:37 INFO spark.storage.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 14.7 KB, free 118.2 KB)\n17/01/19 16:39:37 INFO spark.storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.143.133.233:45916 (size: 14.7 KB, free: 909.0 MB)\n17/01/19 16:39:37 INFO apache.spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:37 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[300] at flatMap at ALS.scala:1170)\n17/01/19 16:39:37 INFO cluster.ego.EGODeployScheduler: Adding task set 44.0 with 2 tasks\n17/01/19 16:39:37 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 44.0 (TID 91, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:37 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 44.0 (TID 92, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:37 INFO spark.storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 14.7 KB, free: 4.2 GB)\n17/01/19 16:39:37 INFO spark.storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 14.7 KB, free: 4.2 GB)\n17/01/19 16:39:37 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:37 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 15 is 236 bytes\n17/01/19 16:39:37 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:37 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 44.0 (TID 91) in 868 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:38 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 44.0 (TID 92) in 889 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:38 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 44.0, whose tasks have all completed, from pool \n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 44 (flatMap at ALS.scala:1170) finished in 0.890 s\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 45, ShuffleMapStage 46)\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:38 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(44)\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[309] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:38 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(45)\n17/01/19 16:39:38 INFO spark.storage.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 45.8 KB, free 164.0 KB)\n17/01/19 16:39:38 INFO spark.storage.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 14.8 KB, free 178.8 KB)\n17/01/19 16:39:38 INFO spark.storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.143.133.233:45916 (size: 14.8 KB, free: 909.0 MB)\n17/01/19 16:39:38 INFO apache.spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[309] at flatMap at ALS.scala:1170)\n17/01/19 16:39:38 INFO cluster.ego.EGODeployScheduler: Adding task set 45.0 with 2 tasks\n17/01/19 16:39:38 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 45.0 (TID 93, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:38 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 94, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:38 INFO spark.storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 14.8 KB, free: 4.2 GB)\n17/01/19 16:39:38 INFO spark.storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 14.8 KB, free: 4.2 GB)\n17/01/19 16:39:38 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:38 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:38 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 14 is 236 bytes\n17/01/19 16:39:38 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 14 is 236 bytes\n17/01/19 16:39:38 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 45.0 (TID 93) in 913 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:38 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 94) in 936 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:38 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 45 (flatMap at ALS.scala:1170) finished in 0.937 s\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 46)\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:38 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(45)\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 46 (MapPartitionsRDD[318] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:38 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(46)\n17/01/19 16:39:38 INFO spark.storage.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 46.7 KB, free 225.5 KB)\n17/01/19 16:39:38 INFO spark.storage.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 15.0 KB, free 240.5 KB)\n17/01/19 16:39:38 INFO spark.storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.143.133.233:45916 (size: 15.0 KB, free: 908.9 MB)\n17/01/19 16:39:38 INFO apache.spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:38 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 46 (MapPartitionsRDD[318] at flatMap at ALS.scala:1170)\n17/01/19 16:39:38 INFO cluster.ego.EGODeployScheduler: Adding task set 46.0 with 2 tasks\n17/01/19 16:39:38 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 46.0 (TID 95, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:38 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 96, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:38 INFO spark.storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 15.0 KB, free: 4.2 GB)\n17/01/19 16:39:38 INFO spark.storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 15.0 KB, free: 4.2 GB)\n17/01/19 16:39:38 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:38 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:38 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 13 is 236 bytes\n17/01/19 16:39:38 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 13 is 236 bytes\n17/01/19 16:39:39 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 46.0 (TID 95) in 877 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:39 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 96) in 881 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:39 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \n17/01/19 16:39:39 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 46 (flatMap at ALS.scala:1170) finished in 0.882 s\n17/01/19 16:39:39 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:39 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:39 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50)\n17/01/19 16:39:39 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(46)\n17/01/19 16:39:39 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:39 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[327] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:39 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(47)\n17/01/19 16:39:39 INFO spark.storage.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 47.6 KB, free 288.1 KB)\n17/01/19 16:39:39 INFO spark.storage.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 15.1 KB, free 303.3 KB)\n17/01/19 16:39:39 INFO spark.storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.143.133.233:45916 (size: 15.1 KB, free: 908.9 MB)\n17/01/19 16:39:39 INFO apache.spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:39 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[327] at flatMap at ALS.scala:1170)\n17/01/19 16:39:39 INFO cluster.ego.EGODeployScheduler: Adding task set 47.0 with 2 tasks\n17/01/19 16:39:39 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 97, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:39 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 47.0 (TID 98, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:39 INFO spark.storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 15.1 KB, free: 4.2 GB)\n17/01/19 16:39:39 INFO spark.storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 15.1 KB, free: 4.2 GB)\n17/01/19 16:39:39 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:39 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 236 bytes\n17/01/19 16:39:39 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:40 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 47.0 (TID 98) in 913 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:40 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 97) in 965 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:40 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \n17/01/19 16:39:40 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 47 (flatMap at ALS.scala:1170) finished in 0.967 s\n17/01/19 16:39:40 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:40 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:40 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(47)\n17/01/19 16:39:40 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50)\n17/01/19 16:39:40 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:40 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 48 (MapPartitionsRDD[336] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:40 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(48)\n17/01/19 16:39:40 INFO spark.storage.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 48.5 KB, free 351.7 KB)\n17/01/19 16:39:40 INFO spark.storage.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 15.3 KB, free 367.0 KB)\n17/01/19 16:39:40 INFO spark.storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.143.133.233:45916 (size: 15.3 KB, free: 908.9 MB)\n17/01/19 16:39:40 INFO apache.spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:40 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 48 (MapPartitionsRDD[336] at flatMap at ALS.scala:1170)\n17/01/19 16:39:40 INFO cluster.ego.EGODeployScheduler: Adding task set 48.0 with 2 tasks\n17/01/19 16:39:40 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 48.0 (TID 99, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:40 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 48.0 (TID 100, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:40 INFO spark.storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 15.3 KB, free: 4.2 GB)\n17/01/19 16:39:40 INFO spark.storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 15.3 KB, free: 4.2 GB)\n17/01/19 16:39:40 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:40 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 11 is 236 bytes\n17/01/19 16:39:40 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:41 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 48.0 (TID 99) in 869 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:41 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 48.0 (TID 100) in 916 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:41 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 48.0, whose tasks have all completed, from pool \n17/01/19 16:39:41 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 48 (flatMap at ALS.scala:1170) finished in 0.918 s\n17/01/19 16:39:41 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:41 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:41 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(48)\n17/01/19 16:39:41 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 49, ShuffleMapStage 50)\n17/01/19 16:39:41 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:41 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 49 (MapPartitionsRDD[345] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:41 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(49)\n17/01/19 16:39:41 INFO spark.storage.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 49.3 KB, free 416.4 KB)\n17/01/19 16:39:41 INFO spark.storage.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 15.4 KB, free 431.8 KB)\n17/01/19 16:39:41 INFO spark.storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.143.133.233:45916 (size: 15.4 KB, free: 908.9 MB)\n17/01/19 16:39:41 INFO apache.spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:41 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[345] at flatMap at ALS.scala:1170)\n17/01/19 16:39:41 INFO cluster.ego.EGODeployScheduler: Adding task set 49.0 with 2 tasks\n17/01/19 16:39:41 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 101, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:41 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 49.0 (TID 102, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:41 INFO spark.storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 15.4 KB, free: 4.2 GB)\n17/01/19 16:39:41 INFO spark.storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 15.4 KB, free: 4.2 GB)\n17/01/19 16:39:41 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:41 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 10 is 236 bytes\n17/01/19 16:39:41 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:42 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 101) in 950 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:42 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 49.0 (TID 102) in 950 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:42 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \n17/01/19 16:39:42 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 49 (flatMap at ALS.scala:1170) finished in 0.952 s\n17/01/19 16:39:42 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:42 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:42 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55, ShuffleMapStage 50)\n17/01/19 16:39:42 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:42 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(49)\n17/01/19 16:39:42 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 50 (MapPartitionsRDD[354] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:42 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(50)\n17/01/19 16:39:42 INFO spark.storage.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 50.2 KB, free 482.0 KB)\n17/01/19 16:39:42 INFO spark.storage.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 15.6 KB, free 497.6 KB)\n17/01/19 16:39:42 INFO spark.storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.143.133.233:45916 (size: 15.6 KB, free: 908.9 MB)\n17/01/19 16:39:42 INFO apache.spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:42 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[354] at flatMap at ALS.scala:1170)\n17/01/19 16:39:42 INFO cluster.ego.EGODeployScheduler: Adding task set 50.0 with 2 tasks\n17/01/19 16:39:42 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 103, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:42 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 50.0 (TID 104, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:42 INFO spark.storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 15.6 KB, free: 4.2 GB)\n17/01/19 16:39:42 INFO spark.storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 15.6 KB, free: 4.2 GB)\n17/01/19 16:39:42 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:42 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 236 bytes\n17/01/19 16:39:42 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:43 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 103) in 887 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:43 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 50.0 (TID 104) in 943 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:43 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \n17/01/19 16:39:43 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 50 (flatMap at ALS.scala:1170) finished in 0.944 s\n17/01/19 16:39:43 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:43 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:43 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55)\n17/01/19 16:39:43 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:43 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(50)\n17/01/19 16:39:43 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[363] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:43 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(51)\n17/01/19 16:39:43 INFO spark.storage.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 51.1 KB, free 548.7 KB)\n17/01/19 16:39:43 INFO spark.storage.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 15.7 KB, free 564.4 KB)\n17/01/19 16:39:43 INFO spark.storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.143.133.233:45916 (size: 15.7 KB, free: 908.9 MB)\n17/01/19 16:39:43 INFO apache.spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:43 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[363] at flatMap at ALS.scala:1170)\n17/01/19 16:39:43 INFO cluster.ego.EGODeployScheduler: Adding task set 51.0 with 2 tasks\n17/01/19 16:39:43 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 105, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:43 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 51.0 (TID 106, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:43 INFO spark.storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 15.7 KB, free: 4.2 GB)\n17/01/19 16:39:43 INFO spark.storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 15.7 KB, free: 4.2 GB)\n17/01/19 16:39:43 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:43 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 236 bytes\n17/01/19 16:39:43 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:44 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 51.0 (TID 106) in 923 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:44 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 105) in 928 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:44 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \n17/01/19 16:39:44 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 51 (flatMap at ALS.scala:1170) finished in 0.928 s\n17/01/19 16:39:44 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:44 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:44 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55)\n17/01/19 16:39:44 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:44 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(51)\n17/01/19 16:39:44 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 52 (MapPartitionsRDD[372] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:44 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(52)\n17/01/19 16:39:44 INFO spark.storage.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 52.0 KB, free 616.4 KB)\n17/01/19 16:39:44 INFO spark.storage.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 15.9 KB, free 632.3 KB)\n17/01/19 16:39:44 INFO spark.storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.143.133.233:45916 (size: 15.9 KB, free: 908.9 MB)\n17/01/19 16:39:44 INFO apache.spark.SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:44 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 52 (MapPartitionsRDD[372] at flatMap at ALS.scala:1170)\n17/01/19 16:39:44 INFO cluster.ego.EGODeployScheduler: Adding task set 52.0 with 2 tasks\n17/01/19 16:39:44 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 52.0 (TID 107, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:44 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 52.0 (TID 108, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:44 INFO spark.storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 15.9 KB, free: 4.2 GB)\n17/01/19 16:39:44 INFO spark.storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 15.9 KB, free: 4.2 GB)\n17/01/19 16:39:44 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:44 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 236 bytes\n17/01/19 16:39:44 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:45 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 52.0 (TID 107) in 873 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:45 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 52.0 (TID 108) in 885 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:45 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 52.0, whose tasks have all completed, from pool \n17/01/19 16:39:45 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 52 (flatMap at ALS.scala:1170) finished in 0.885 s\n17/01/19 16:39:45 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:45 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:45 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(52)\n17/01/19 16:39:45 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 53, ShuffleMapStage 54, ResultStage 55)\n17/01/19 16:39:45 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:45 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 53 (MapPartitionsRDD[381] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:45 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(53)\n17/01/19 16:39:45 INFO spark.storage.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 52.9 KB, free 685.2 KB)\n17/01/19 16:39:45 INFO spark.storage.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 16.0 KB, free 701.2 KB)\n17/01/19 16:39:45 INFO spark.storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.143.133.233:45916 (size: 16.0 KB, free: 908.8 MB)\n17/01/19 16:39:45 INFO apache.spark.SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:45 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 53 (MapPartitionsRDD[381] at flatMap at ALS.scala:1170)\n17/01/19 16:39:45 INFO cluster.ego.EGODeployScheduler: Adding task set 53.0 with 2 tasks\n17/01/19 16:39:45 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 53.0 (TID 109, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:45 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 53.0 (TID 110, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:45 INFO spark.storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 16.0 KB, free: 4.2 GB)\n17/01/19 16:39:45 INFO spark.storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 16.0 KB, free: 4.2 GB)\n17/01/19 16:39:45 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:45 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:45 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 236 bytes\n17/01/19 16:39:45 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 236 bytes\n17/01/19 16:39:46 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 53.0 (TID 109) in 913 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:46 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 53.0 (TID 110) in 939 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:46 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \n17/01/19 16:39:46 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 53 (flatMap at ALS.scala:1170) finished in 0.941 s\n17/01/19 16:39:46 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:46 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:46 INFO spark.scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 54, ResultStage 55)\n17/01/19 16:39:46 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(53)\n17/01/19 16:39:46 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:46 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[390] at flatMap at ALS.scala:1170), which has no missing parents\n17/01/19 16:39:46 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(54)\n17/01/19 16:39:46 INFO spark.storage.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 53.7 KB, free 754.9 KB)\n17/01/19 16:39:46 INFO spark.storage.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 16.2 KB, free 771.1 KB)\n17/01/19 16:39:46 INFO spark.storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.143.133.233:45916 (size: 16.2 KB, free: 908.8 MB)\n17/01/19 16:39:46 INFO apache.spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:46 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[390] at flatMap at ALS.scala:1170)\n17/01/19 16:39:46 INFO cluster.ego.EGODeployScheduler: Adding task set 54.0 with 2 tasks\n17/01/19 16:39:46 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 54.0 (TID 111, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:46 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 54.0 (TID 112, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:39:46 INFO spark.storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 16.2 KB, free: 4.2 GB)\n17/01/19 16:39:46 INFO spark.storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 16.2 KB, free: 4.2 GB)\n17/01/19 16:39:46 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:46 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 236 bytes\n17/01/19 16:39:46 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:47 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 54.0 (TID 112) in 868 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:47 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 54.0 (TID 111) in 886 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:47 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \n17/01/19 16:39:47 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 54 (flatMap at ALS.scala:1170) finished in 0.888 s\n17/01/19 16:39:47 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:47 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:47 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(54)\n17/01/19 16:39:47 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 55)\n17/01/19 16:39:47 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:47 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 55 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255), which has no missing parents\n17/01/19 16:39:47 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(55)\n17/01/19 16:39:47 INFO spark.storage.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 54.9 KB, free 826.1 KB)\n17/01/19 16:39:47 INFO spark.storage.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 16.5 KB, free 842.6 KB)\n17/01/19 16:39:47 INFO spark.storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.143.133.233:45916 (size: 16.5 KB, free: 908.8 MB)\n17/01/19 16:39:47 INFO apache.spark.SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:47 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 55 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255)\n17/01/19 16:39:47 INFO cluster.ego.EGODeployScheduler: Adding task set 55.0 with 2 tasks\n17/01/19 16:39:47 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 55.0 (TID 113, yp-spark-dal09-env5-0033, partition 1,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:39:47 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 55.0 (TID 114, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:39:47 INFO spark.storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 16.5 KB, free: 4.2 GB)\n17/01/19 16:39:47 INFO spark.storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 16.5 KB, free: 4.2 GB)\n17/01/19 16:39:47 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:47 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 4 is 236 bytes\n17/01/19 16:39:47 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:48 INFO spark.storage.BlockManagerInfo: Added rdd_406_1 in memory on yp-spark-dal09-env5-0033:38602 (size: 1347.8 KB, free: 4.2 GB)\n17/01/19 16:39:48 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 55.0 (TID 113) in 1002 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:39:48 INFO spark.storage.BlockManagerInfo: Added rdd_406_0 in memory on yp-spark-dal09-env5-0024:38984 (size: 1347.8 KB, free: 4.2 GB)\n17/01/19 16:39:48 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 55.0 (TID 114) in 1033 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:39:48 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 55.0, whose tasks have all completed, from pool \n17/01/19 16:39:48 INFO spark.scheduler.DAGScheduler: ResultStage 55 (count at ALS.scala:263) finished in 1.034 s\n17/01/19 16:39:48 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(55)\n17/01/19 16:39:48 INFO spark.scheduler.DAGScheduler: Job 5 finished: count at ALS.scala:263, took 38.716247 s\n17/01/19 16:39:48 INFO apache.spark.SparkContext: Starting job: count at ALS.scala:264\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 253 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 253 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 253 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 44 is 239 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 43 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 42 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 41 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 40 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 39 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 38 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 37 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 36 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 35 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 34 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 33 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 32 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 31 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 30 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 29 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 28 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 27 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 26 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 25 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 24 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 23 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 22 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 21 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 20 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 19 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 18 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 17 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 16 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 15 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 14 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 13 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 11 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 10 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 236 bytes\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 236 bytes\n17/01/19 16:39:48 INFO spark.scheduler.DAGScheduler: Got job 6 (count at ALS.scala:264) with 2 output partitions\n17/01/19 16:39:48 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 100 (count at ALS.scala:264)\n17/01/19 16:39:48 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 99, ShuffleMapStage 57)\n17/01/19 16:39:48 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:39:48 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 100 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259), which has no missing parents\n17/01/19 16:39:48 INFO spark.storage.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 54.1 KB, free 896.7 KB)\n17/01/19 16:39:48 INFO spark.storage.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 16.4 KB, free 913.1 KB)\n17/01/19 16:39:48 INFO spark.storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.143.133.233:45916 (size: 16.4 KB, free: 908.8 MB)\n17/01/19 16:39:48 INFO apache.spark.SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:48 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 100 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259)\n17/01/19 16:39:48 INFO cluster.ego.EGODeployScheduler: Adding task set 100.0 with 2 tasks\n17/01/19 16:39:48 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 100.0 (TID 115, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:39:48 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 100.0 (TID 116, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:39:48 INFO spark.storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 16.4 KB, free: 4.2 GB)\n17/01/19 16:39:48 INFO spark.storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 16.4 KB, free: 4.2 GB)\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:48 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:48 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(100)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added rdd_407_1 in memory on yp-spark-dal09-env5-0024:38984 (size: 808.8 KB, free: 4.2 GB)\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 100.0 (TID 116) in 920 ms on yp-spark-dal09-env5-0024 (1/2)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added rdd_407_0 in memory on yp-spark-dal09-env5-0033:38602 (size: 808.8 KB, free: 4.2 GB)\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 100.0 (TID 115) in 925 ms on yp-spark-dal09-env5-0033 (2/2)\n17/01/19 16:39:49 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 100.0, whose tasks have all completed, from pool \n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: ResultStage 100 (count at ALS.scala:264) finished in 0.926 s\n17/01/19 16:39:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(100)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Job 6 finished: count at ALS.scala:264, took 0.955298 s\n17/01/19 16:39:49 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Got job 7 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 146 (first at MatrixFactorizationModel.scala:67)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 104, ShuffleMapStage 145)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 146 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255), which has no missing parents\n17/01/19 16:39:49 INFO spark.storage.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 55.1 KB, free 968.2 KB)\n17/01/19 16:39:49 INFO spark.storage.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 16.6 KB, free 984.8 KB)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.143.133.233:45916 (size: 16.6 KB, free: 908.8 MB)\n17/01/19 16:39:49 INFO apache.spark.SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 146 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255)\n17/01/19 16:39:49 INFO cluster.ego.EGODeployScheduler: Adding task set 146.0 with 1 tasks\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 146.0 (TID 117, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 16.6 KB, free: 4.2 GB)\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 146.0 (TID 117) in 17 ms on yp-spark-dal09-env5-0024 (1/1)\n17/01/19 16:39:49 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 146.0, whose tasks have all completed, from pool \n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: ResultStage 146 (first at MatrixFactorizationModel.scala:67) finished in 0.017 s\n17/01/19 16:39:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(146)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Job 7 finished: first at MatrixFactorizationModel.scala:67, took 0.038678 s\n17/01/19 16:39:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(146)\n17/01/19 16:39:49 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Got job 8 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 191 (first at MatrixFactorizationModel.scala:67)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 190, ShuffleMapStage 148)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 191 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259), which has no missing parents\n17/01/19 16:39:49 INFO spark.storage.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 54.2 KB, free 1039.0 KB)\n17/01/19 16:39:49 INFO spark.storage.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 16.5 KB, free 1055.5 KB)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.143.133.233:45916 (size: 16.5 KB, free: 908.8 MB)\n17/01/19 16:39:49 INFO apache.spark.SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 191 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259)\n17/01/19 16:39:49 INFO cluster.ego.EGODeployScheduler: Adding task set 191.0 with 1 tasks\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 191.0 (TID 118, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 16.5 KB, free: 4.2 GB)\n17/01/19 16:39:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(191)\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 191.0 (TID 118) in 15 ms on yp-spark-dal09-env5-0033 (1/1)\n17/01/19 16:39:49 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 191.0, whose tasks have all completed, from pool \n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: ResultStage 191 (first at MatrixFactorizationModel.scala:67) finished in 0.017 s\n17/01/19 16:39:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(191)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Job 8 finished: first at MatrixFactorizationModel.scala:67, took 0.036335 s\n17/01/19 16:39:49 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Got job 9 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 237 (first at MatrixFactorizationModel.scala:67)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 195, ShuffleMapStage 236)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 237 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255), which has no missing parents\n17/01/19 16:39:49 INFO spark.storage.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 55.1 KB, free 1110.6 KB)\n17/01/19 16:39:49 INFO spark.storage.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 16.6 KB, free 1127.2 KB)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.143.133.233:45916 (size: 16.6 KB, free: 908.7 MB)\n17/01/19 16:39:49 INFO apache.spark.SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 237 (users MapPartitionsRDD[406] at mapValues at ALS.scala:255)\n17/01/19 16:39:49 INFO cluster.ego.EGODeployScheduler: Adding task set 237.0 with 1 tasks\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 237.0 (TID 119, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 16.6 KB, free: 4.2 GB)\n17/01/19 16:39:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(237)\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 237.0 (TID 119) in 11 ms on yp-spark-dal09-env5-0024 (1/1)\n17/01/19 16:39:49 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 237.0, whose tasks have all completed, from pool \n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: ResultStage 237 (first at MatrixFactorizationModel.scala:67) finished in 0.011 s\n17/01/19 16:39:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(237)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Job 9 finished: first at MatrixFactorizationModel.scala:67, took 0.029552 s\n17/01/19 16:39:49 INFO apache.spark.SparkContext: Starting job: first at MatrixFactorizationModel.scala:67\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Got job 10 (first at MatrixFactorizationModel.scala:67) with 1 output partitions\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 282 (first at MatrixFactorizationModel.scala:67)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 281, ShuffleMapStage 239)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 282 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259), which has no missing parents\n17/01/19 16:39:49 INFO spark.storage.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 54.2 KB, free 1181.4 KB)\n17/01/19 16:39:49 INFO spark.storage.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 16.5 KB, free 1197.9 KB)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.143.133.233:45916 (size: 16.5 KB, free: 908.7 MB)\n17/01/19 16:39:49 INFO apache.spark.SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 282 (products MapPartitionsRDD[407] at mapValues at ALS.scala:259)\n17/01/19 16:39:49 INFO cluster.ego.EGODeployScheduler: Adding task set 282.0 with 1 tasks\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 282.0 (TID 120, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 16.5 KB, free: 4.2 GB)\n17/01/19 16:39:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(282)\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 282.0 (TID 120) in 11 ms on yp-spark-dal09-env5-0033 (1/1)\n17/01/19 16:39:49 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 282.0, whose tasks have all completed, from pool \n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: ResultStage 282 (first at MatrixFactorizationModel.scala:67) finished in 0.012 s\n17/01/19 16:39:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(282)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Job 10 finished: first at MatrixFactorizationModel.scala:67, took 0.029765 s\n17/01/19 16:39:49 INFO CloudantRecommender: [Finished train model: , 2017-01-19 16:39:49 CST]\n17/01/19 16:39:49 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-19 16:39:49 CST]\n17/01/19 16:39:49 INFO apache.spark.SparkContext: Starting job: runJob at PythonRDD.scala:393\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Registering RDD 411 (flatMap at MatrixFactorizationModel.scala:278)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Got job 11 (runJob at PythonRDD.scala:393) with 1 output partitions\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 372 (runJob at PythonRDD.scala:393)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 371)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 371)\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 371 (MapPartitionsRDD[411] at flatMap at MatrixFactorizationModel.scala:278), which has no missing parents\n17/01/19 16:39:49 INFO spark.storage.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 58.4 KB, free 1256.3 KB)\n17/01/19 16:39:49 INFO spark.storage.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 17.9 KB, free 1274.2 KB)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.143.133.233:45916 (size: 17.9 KB, free: 908.7 MB)\n17/01/19 16:39:49 INFO apache.spark.SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:49 INFO spark.scheduler.DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 371 (MapPartitionsRDD[411] at flatMap at MatrixFactorizationModel.scala:278)\n17/01/19 16:39:49 INFO cluster.ego.EGODeployScheduler: Adding task set 371.0 with 4 tasks\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 371.0 (TID 121, yp-spark-dal09-env5-0024, partition 0,PROCESS_LOCAL, 2519 bytes)\n17/01/19 16:39:49 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 371.0 (TID 122, yp-spark-dal09-env5-0033, partition 2,PROCESS_LOCAL, 2519 bytes)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 17.9 KB, free: 4.2 GB)\n17/01/19 16:39:49 INFO spark.storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 17.9 KB, free: 4.2 GB)\n17/01/19 16:39:49 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(371)\n17/01/19 16:39:51 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 371.0 (TID 123, yp-spark-dal09-env5-0033, partition 3,PROCESS_LOCAL, 2519 bytes)\n17/01/19 16:39:54 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 371.0 (TID 124, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2519 bytes)\n17/01/19 16:39:54 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 371.0 (TID 121) in 5307 ms on yp-spark-dal09-env5-0024 (1/4)\n17/01/19 16:39:55 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 371.0 (TID 122) in 5731 ms on yp-spark-dal09-env5-0033 (2/4)\n17/01/19 16:39:55 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 371.0 (TID 123) in 4538 ms on yp-spark-dal09-env5-0033 (3/4)\n17/01/19 16:39:58 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 371.0 (TID 124) in 3944 ms on yp-spark-dal09-env5-0024 (4/4)\n17/01/19 16:39:58 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 371.0, whose tasks have all completed, from pool \n17/01/19 16:39:58 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 371 (flatMap at MatrixFactorizationModel.scala:278) finished in 9.252 s\n17/01/19 16:39:58 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:39:58 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:39:58 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 372)\n17/01/19 16:39:58 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:39:58 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(371)\n17/01/19 16:39:58 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 372 (PythonRDD[417] at RDD at PythonRDD.scala:43), which has no missing parents\n17/01/19 16:39:58 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(372)\n17/01/19 16:39:58 INFO spark.storage.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 60.5 KB, free 1334.7 KB)\n17/01/19 16:39:58 INFO spark.storage.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 18.9 KB, free 1353.6 KB)\n17/01/19 16:39:58 INFO spark.storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.143.133.233:45916 (size: 18.9 KB, free: 908.7 MB)\n17/01/19 16:39:58 INFO apache.spark.SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:58 INFO spark.scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 372 (PythonRDD[417] at RDD at PythonRDD.scala:43)\n17/01/19 16:39:58 INFO cluster.ego.EGODeployScheduler: Adding task set 372.0 with 1 tasks\n17/01/19 16:39:58 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 372.0 (TID 125, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1894 bytes)\n17/01/19 16:39:58 INFO spark.storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 18.9 KB, free: 4.2 GB)\n17/01/19 16:39:58 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 45 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:39:58 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 45 is 244 bytes\n17/01/19 16:39:59 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 372.0 (TID 125) in 333 ms on yp-spark-dal09-env5-0024 (1/1)\n17/01/19 16:39:59 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 372.0, whose tasks have all completed, from pool \n17/01/19 16:39:59 INFO spark.scheduler.DAGScheduler: ResultStage 372 (runJob at PythonRDD.scala:393) finished in 0.333 s\n17/01/19 16:39:59 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(372)\n17/01/19 16:39:59 INFO spark.scheduler.DAGScheduler: Job 11 finished: runJob at PythonRDD.scala:393, took 9.620406 s\n17/01/19 16:39:59 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-19 16:39:59 CST]\n17/01/19 16:39:59 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1484865599]\n17/01/19 16:39:59 INFO apache.spark.SparkContext: Starting job: collect at <ipython-input-10-5997fdef2cd1>:145\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 253 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 253 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 253 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 44 is 239 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 43 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 42 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 41 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 40 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 39 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 38 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 37 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 36 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 35 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 34 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 33 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 32 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 31 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 30 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 29 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 28 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 27 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 26 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 25 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 24 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 23 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 22 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 21 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 20 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 19 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 18 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 17 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 16 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 15 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 14 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 13 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 11 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 10 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 236 bytes\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 4 is 236 bytes\n17/01/19 16:39:59 INFO spark.scheduler.DAGScheduler: Got job 12 (collect at <ipython-input-10-5997fdef2cd1>:145) with 4 output partitions\n17/01/19 16:39:59 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 462 (collect at <ipython-input-10-5997fdef2cd1>:145)\n17/01/19 16:39:59 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 461)\n17/01/19 16:39:59 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:39:59 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 462 (PythonRDD[427] at collect at <ipython-input-10-5997fdef2cd1>:145), which has no missing parents\n17/01/19 16:39:59 INFO spark.storage.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 71.9 KB, free 1425.5 KB)\n17/01/19 16:39:59 INFO spark.storage.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 24.7 KB, free 1450.2 KB)\n17/01/19 16:39:59 INFO spark.storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.143.133.233:45916 (size: 24.7 KB, free: 908.7 MB)\n17/01/19 16:39:59 INFO apache.spark.SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:39:59 INFO spark.scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 462 (PythonRDD[427] at collect at <ipython-input-10-5997fdef2cd1>:145)\n17/01/19 16:39:59 INFO cluster.ego.EGODeployScheduler: Adding task set 462.0 with 4 tasks\n17/01/19 16:39:59 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 462.0 (TID 126, yp-spark-dal09-env5-0033, partition 1,NODE_LOCAL, 1894 bytes)\n17/01/19 16:39:59 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 462.0 (TID 127, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1894 bytes)\n17/01/19 16:39:59 INFO spark.storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 24.7 KB, free: 4.2 GB)\n17/01/19 16:39:59 INFO spark.storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 24.7 KB, free: 4.2 GB)\n17/01/19 16:39:59 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(462)\n17/01/19 16:39:59 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 45 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:39:59 INFO spark.storage.BlockManagerInfo: Added rdd_423_1 in memory on yp-spark-dal09-env5-0033:38602 (size: 619.7 KB, free: 4.2 GB)\n17/01/19 16:39:59 INFO spark.storage.BlockManagerInfo: Added rdd_423_0 in memory on yp-spark-dal09-env5-0024:38984 (size: 619.7 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.scheduler.TaskSetManager: Starting task 3.0 in stage 462.0 (TID 128, yp-spark-dal09-env5-0033, partition 3,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:00 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 462.0 (TID 126) in 742 ms on yp-spark-dal09-env5-0033 (1/4)\n17/01/19 16:40:00 INFO spark.scheduler.TaskSetManager: Starting task 2.0 in stage 462.0 (TID 129, yp-spark-dal09-env5-0024, partition 2,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:00 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 462.0 (TID 127) in 812 ms on yp-spark-dal09-env5-0024 (2/4)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Added rdd_423_3 in memory on yp-spark-dal09-env5-0033:38602 (size: 619.7 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Added rdd_423_2 in memory on yp-spark-dal09-env5-0024:38984 (size: 619.7 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.143.133.233:45916 in memory (size: 18.9 KB, free: 908.7 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_58_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 18.9 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.scheduler.TaskSetManager: Finished task 3.0 in stage 462.0 (TID 128) in 490 ms on yp-spark-dal09-env5-0033 (3/4)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 69\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_57_piece0 on 10.143.133.233:45916 in memory (size: 17.9 KB, free: 908.7 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_57_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 17.9 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_57_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 17.9 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 68\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_56_piece0 on 10.143.133.233:45916 in memory (size: 16.5 KB, free: 908.7 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_56_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 16.5 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 67\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.143.133.233:45916 in memory (size: 16.6 KB, free: 908.7 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_55_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 16.6 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 66\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.143.133.233:45916 in memory (size: 16.5 KB, free: 908.7 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_54_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 16.5 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 65\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.143.133.233:45916 in memory (size: 16.6 KB, free: 908.8 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_53_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 16.6 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 64\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.143.133.233:45916 in memory (size: 16.4 KB, free: 908.8 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_52_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 16.4 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_52_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 16.4 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 63\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.143.133.233:45916 in memory (size: 16.5 KB, free: 908.8 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_51_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 16.5 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_51_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 16.5 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 62\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.143.133.233:45916 in memory (size: 16.2 KB, free: 908.8 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_50_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 16.2 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_50_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 16.2 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 61\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.143.133.233:45916 in memory (size: 16.0 KB, free: 908.8 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_49_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 16.0 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_49_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 16.0 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 60\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.143.133.233:45916 in memory (size: 15.9 KB, free: 908.8 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_48_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 15.9 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_48_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 15.9 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 59\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.143.133.233:45916 in memory (size: 15.7 KB, free: 908.9 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_47_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 15.7 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_47_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 15.7 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 58\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.143.133.233:45916 in memory (size: 15.6 KB, free: 908.9 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_46_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 15.6 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_46_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 15.6 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 57\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.143.133.233:45916 in memory (size: 15.4 KB, free: 908.9 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_45_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 15.4 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_45_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 15.4 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 56\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.143.133.233:45916 in memory (size: 15.3 KB, free: 908.9 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_44_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 15.3 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_44_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 15.3 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 55\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.143.133.233:45916 in memory (size: 15.1 KB, free: 908.9 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_43_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 15.1 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_43_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 15.1 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 54\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.143.133.233:45916 in memory (size: 15.0 KB, free: 908.9 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_42_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 15.0 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_42_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 15.0 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 53\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.143.133.233:45916 in memory (size: 14.8 KB, free: 908.9 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_41_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 14.8 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_41_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 14.8 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 52\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.143.133.233:45916 in memory (size: 14.7 KB, free: 909.0 MB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_40_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 14.7 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_40_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 14.7 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 51\n17/01/19 16:40:00 INFO spark.scheduler.TaskSetManager: Finished task 2.0 in stage 462.0 (TID 129) in 468 ms on yp-spark-dal09-env5-0024 (4/4)\n17/01/19 16:40:00 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 462.0, whose tasks have all completed, from pool \n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.143.133.233:45916 in memory (size: 14.5 KB, free: 909.0 MB)\n17/01/19 16:40:00 INFO spark.scheduler.DAGScheduler: ResultStage 462 (collect at <ipython-input-10-5997fdef2cd1>:145) finished in 1.282 s\n17/01/19 16:40:00 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(462)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_39_piece0 on yp-spark-dal09-env5-0033:38602 in memory (size: 14.5 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.storage.BlockManagerInfo: Removed broadcast_39_piece0 on yp-spark-dal09-env5-0024:38984 in memory (size: 14.5 KB, free: 4.2 GB)\n17/01/19 16:40:00 INFO spark.scheduler.DAGScheduler: Job 12 finished: collect at <ipython-input-10-5997fdef2cd1>:145, took 1.314136 s\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 50\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 49\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 48\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 47\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 46\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 45\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 44\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 43\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 42\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 41\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 40\n17/01/19 16:40:00 INFO apache.spark.ContextCleaner: Cleaned accumulator 39\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-19 16:40:01 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-19 16:40:03 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-19 16:40:06 CST]\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-19 16:40:08 CST]\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-19 16:40:08 CST]\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-19 16:40:08 CST]\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-19 16:40:08 CST]\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-19 16:40:08 CST]\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-19 16:40:08 CST]\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-19 16:40:08 CST]\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-19 16:40:08 CST]\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-19 16:40:08 CST]\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-19 16:40:08 CST]\n17/01/19 16:40:09 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-19 16:40:09 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-19 16:40:11 CST]\n17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-19 16:40:13 CST]\n17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-19 16:40:13 CST]\n17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-19 16:40:13 CST]\n17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-19 16:40:13 CST]\n17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendationdb metadata record, {'timestamp_utc': '2017-01-19T22:40:13.807491', '_id': 'recommendation_metadata', 'latest_db': 'recommendationdb_1484865599'}]\n17/01/19 16:40:13 INFO apache.spark.SparkContext: Starting job: sortByKey at <ipython-input-10-5997fdef2cd1>:102\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Got job 13 (sortByKey at <ipython-input-10-5997fdef2cd1>:102) with 2 output partitions\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 507 (sortByKey at <ipython-input-10-5997fdef2cd1>:102)\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 506, ShuffleMapStage 464)\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 507 (PythonRDD[431] at sortByKey at <ipython-input-10-5997fdef2cd1>:102), which has no missing parents\n17/01/19 16:40:13 INFO spark.storage.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 57.0 KB, free 153.6 KB)\n17/01/19 16:40:13 INFO spark.storage.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 18.0 KB, free 171.6 KB)\n17/01/19 16:40:13 INFO spark.storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.143.133.233:45916 (size: 18.0 KB, free: 909.0 MB)\n17/01/19 16:40:13 INFO apache.spark.SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 507 (PythonRDD[431] at sortByKey at <ipython-input-10-5997fdef2cd1>:102)\n17/01/19 16:40:13 INFO cluster.ego.EGODeployScheduler: Adding task set 507.0 with 2 tasks\n17/01/19 16:40:13 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 507.0 (TID 130, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:40:13 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 507.0 (TID 131, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:40:13 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(507)\n17/01/19 16:40:13 INFO spark.storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 18.0 KB, free: 4.2 GB)\n17/01/19 16:40:13 INFO spark.storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 18.0 KB, free: 4.2 GB)\n17/01/19 16:40:13 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 507.0 (TID 130) in 84 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:13 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 507.0 (TID 131) in 87 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:13 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 507.0, whose tasks have all completed, from pool \n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: ResultStage 507 (sortByKey at <ipython-input-10-5997fdef2cd1>:102) finished in 0.087 s\n17/01/19 16:40:13 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(507)\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Job 13 finished: sortByKey at <ipython-input-10-5997fdef2cd1>:102, took 0.102368 s\n17/01/19 16:40:13 INFO apache.spark.SparkContext: Starting job: sortByKey at <ipython-input-10-5997fdef2cd1>:102\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Got job 14 (sortByKey at <ipython-input-10-5997fdef2cd1>:102) with 2 output partitions\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 552 (sortByKey at <ipython-input-10-5997fdef2cd1>:102)\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 509, ShuffleMapStage 551)\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:40:13 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 552 (PythonRDD[432] at sortByKey at <ipython-input-10-5997fdef2cd1>:102), which has no missing parents\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 56.7 KB, free 228.3 KB)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 18.0 KB, free 246.4 KB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.143.133.233:45916 (size: 18.0 KB, free: 908.9 MB)\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 552 (PythonRDD[432] at sortByKey at <ipython-input-10-5997fdef2cd1>:102)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Adding task set 552.0 with 2 tasks\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 552.0 (TID 132, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 552.0 (TID 133, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2198 bytes)\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(552)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 18.0 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 18.0 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 552.0 (TID 133) in 67 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 552.0 (TID 132) in 73 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 552.0, whose tasks have all completed, from pool \n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: ResultStage 552 (sortByKey at <ipython-input-10-5997fdef2cd1>:102) finished in 0.073 s\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(552)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Job 14 finished: sortByKey at <ipython-input-10-5997fdef2cd1>:102, took 0.088802 s\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Starting job: sortByKey at <ipython-input-10-5997fdef2cd1>:104\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Registering RDD 434 (sortByKey at <ipython-input-10-5997fdef2cd1>:102)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Got job 15 (sortByKey at <ipython-input-10-5997fdef2cd1>:104) with 2 output partitions\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 598 (sortByKey at <ipython-input-10-5997fdef2cd1>:104)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 597)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 597)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 597 (PairwiseRDD[434] at sortByKey at <ipython-input-10-5997fdef2cd1>:102), which has no missing parents\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 57.7 KB, free 304.1 KB)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 18.7 KB, free 322.8 KB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.143.133.233:45916 (size: 18.7 KB, free: 908.9 MB)\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 597 (PairwiseRDD[434] at sortByKey at <ipython-input-10-5997fdef2cd1>:102)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Adding task set 597.0 with 2 tasks\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 597.0 (TID 134, yp-spark-dal09-env5-0024, partition 1,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 597.0 (TID 135, yp-spark-dal09-env5-0033, partition 0,PROCESS_LOCAL, 2187 bytes)\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(597)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 18.7 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 18.7 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 597.0 (TID 135) in 80 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 597.0 (TID 134) in 89 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 597.0, whose tasks have all completed, from pool \n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 597 (sortByKey at <ipython-input-10-5997fdef2cd1>:102) finished in 0.089 s\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 598)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(597)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 598 (PythonRDD[437] at sortByKey at <ipython-input-10-5997fdef2cd1>:104), which has no missing parents\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(598)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 6.1 KB, free 328.9 KB)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 3.8 KB, free 332.7 KB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.143.133.233:45916 (size: 3.8 KB, free: 908.9 MB)\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 598 (PythonRDD[437] at sortByKey at <ipython-input-10-5997fdef2cd1>:104)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Adding task set 598.0 with 2 tasks\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 598.0 (TID 136, yp-spark-dal09-env5-0033, partition 0,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 598.0 (TID 137, yp-spark-dal09-env5-0024, partition 1,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 3.8 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 3.8 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 46 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 46 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 46 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 598.0 (TID 136) in 16 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 598.0 (TID 137) in 17 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 598.0, whose tasks have all completed, from pool \n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: ResultStage 598 (sortByKey at <ipython-input-10-5997fdef2cd1>:104) finished in 0.017 s\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(598)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Job 15 finished: sortByKey at <ipython-input-10-5997fdef2cd1>:104, took 0.126312 s\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Starting job: sortByKey at <ipython-input-10-5997fdef2cd1>:104\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 253 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 253 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 253 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 44 is 239 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 43 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 42 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 41 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 40 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 39 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 38 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 37 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 36 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 35 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 34 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 33 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 32 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 31 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 30 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 29 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 28 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 27 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 26 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 25 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 24 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 23 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 22 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 21 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 20 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 19 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 18 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 17 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 16 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 15 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 14 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 13 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 11 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 10 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 236 bytes\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Got job 16 (sortByKey at <ipython-input-10-5997fdef2cd1>:104) with 2 output partitions\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 644 (sortByKey at <ipython-input-10-5997fdef2cd1>:104)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 643)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 644 (PythonRDD[438] at sortByKey at <ipython-input-10-5997fdef2cd1>:104), which has no missing parents\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 6.1 KB, free 338.8 KB)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 3.9 KB, free 342.7 KB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.143.133.233:45916 (size: 3.9 KB, free: 908.9 MB)\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 644 (PythonRDD[438] at sortByKey at <ipython-input-10-5997fdef2cd1>:104)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Adding task set 644.0 with 2 tasks\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 644.0 (TID 138, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 644.0 (TID 139, yp-spark-dal09-env5-0033, partition 1,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 3.9 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(644)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 3.9 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 644.0 (TID 139) in 13 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 644.0 (TID 138) in 99 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 644.0, whose tasks have all completed, from pool \n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: ResultStage 644 (sortByKey at <ipython-input-10-5997fdef2cd1>:104) finished in 0.099 s\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(644)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Job 16 finished: sortByKey at <ipython-input-10-5997fdef2cd1>:104, took 0.117754 s\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Starting job: collect at <ipython-input-10-5997fdef2cd1>:104\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Registering RDD 440 (sortByKey at <ipython-input-10-5997fdef2cd1>:104)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Got job 17 (collect at <ipython-input-10-5997fdef2cd1>:104) with 2 output partitions\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 691 (collect at <ipython-input-10-5997fdef2cd1>:104)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 690)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 690)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 690 (PairwiseRDD[440] at sortByKey at <ipython-input-10-5997fdef2cd1>:104), which has no missing parents\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 6.7 KB, free 349.4 KB)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 4.3 KB, free 353.7 KB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.143.133.233:45916 (size: 4.3 KB, free: 908.9 MB)\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 690 (PairwiseRDD[440] at sortByKey at <ipython-input-10-5997fdef2cd1>:104)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Adding task set 690.0 with 2 tasks\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 690.0 (TID 140, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1883 bytes)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 690.0 (TID 141, yp-spark-dal09-env5-0033, partition 1,NODE_LOCAL, 1883 bytes)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 4.3 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 4.3 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(690)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 690.0 (TID 141) in 26 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 690.0 (TID 140) in 26 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 690.0, whose tasks have all completed, from pool \n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 690 (sortByKey at <ipython-input-10-5997fdef2cd1>:104) finished in 0.027 s\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(690)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 691)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 691 (PythonRDD[443] at collect at <ipython-input-10-5997fdef2cd1>:104), which has no missing parents\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(691)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_66 stored as values in memory (estimated size 5.6 KB, free 359.3 KB)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 3.4 KB, free 362.7 KB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.143.133.233:45916 (size: 3.4 KB, free: 908.9 MB)\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 691 (PythonRDD[443] at collect at <ipython-input-10-5997fdef2cd1>:104)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Adding task set 691.0 with 2 tasks\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 691.0 (TID 142, yp-spark-dal09-env5-0033, partition 1,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 691.0 (TID 143, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 3.4 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 3.4 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 47 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 47 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 47 is 239 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 47 is 239 bytes\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 691.0 (TID 142) in 15 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 691.0 (TID 143) in 15 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 691.0, whose tasks have all completed, from pool \n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: ResultStage 691 (collect at <ipython-input-10-5997fdef2cd1>:104) finished in 0.016 s\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(691)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Job 17 finished: collect at <ipython-input-10-5997fdef2cd1>:104, took 0.058190 s\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Starting job: sortByKey at <ipython-input-10-5997fdef2cd1>:105\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 253 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 253 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 253 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 44 is 239 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 43 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 42 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 41 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 40 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 39 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 38 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 37 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 36 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 35 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 34 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 33 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 32 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 31 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 30 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 29 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 28 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 27 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 26 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 25 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 24 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 23 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 22 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 21 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 20 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 19 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 18 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 17 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 16 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 15 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 14 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 13 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 12 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 11 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 10 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 236 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 46 is 236 bytes\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Got job 18 (sortByKey at <ipython-input-10-5997fdef2cd1>:105) with 2 output partitions\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 737 (sortByKey at <ipython-input-10-5997fdef2cd1>:105)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 736)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 737 (PythonRDD[444] at sortByKey at <ipython-input-10-5997fdef2cd1>:105), which has no missing parents\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_67 stored as values in memory (estimated size 6.1 KB, free 368.7 KB)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 3.8 KB, free 372.5 KB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.143.133.233:45916 (size: 3.8 KB, free: 908.9 MB)\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 737 (PythonRDD[444] at sortByKey at <ipython-input-10-5997fdef2cd1>:105)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Adding task set 737.0 with 2 tasks\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 737.0 (TID 144, yp-spark-dal09-env5-0033, partition 0,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 737.0 (TID 145, yp-spark-dal09-env5-0024, partition 1,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 3.8 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 3.8 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 46 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 46 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(737)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 737.0 (TID 144) in 12 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 737.0 (TID 145) in 14 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 737.0, whose tasks have all completed, from pool \n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: ResultStage 737 (sortByKey at <ipython-input-10-5997fdef2cd1>:105) finished in 0.015 s\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(737)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Job 18 finished: sortByKey at <ipython-input-10-5997fdef2cd1>:105, took 0.033117 s\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Starting job: sortByKey at <ipython-input-10-5997fdef2cd1>:105\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Got job 19 (sortByKey at <ipython-input-10-5997fdef2cd1>:105) with 2 output partitions\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 783 (sortByKey at <ipython-input-10-5997fdef2cd1>:105)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 782)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Missing parents: List()\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 783 (PythonRDD[445] at sortByKey at <ipython-input-10-5997fdef2cd1>:105), which has no missing parents\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_68 stored as values in memory (estimated size 6.1 KB, free 378.6 KB)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 3.9 KB, free 382.5 KB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.143.133.233:45916 (size: 3.9 KB, free: 908.9 MB)\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 783 (PythonRDD[445] at sortByKey at <ipython-input-10-5997fdef2cd1>:105)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Adding task set 783.0 with 2 tasks\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 783.0 (TID 146, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 783.0 (TID 147, yp-spark-dal09-env5-0033, partition 1,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 3.9 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 3.9 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(783)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 783.0 (TID 147) in 12 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 783.0 (TID 146) in 14 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 783.0, whose tasks have all completed, from pool \n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: ResultStage 783 (sortByKey at <ipython-input-10-5997fdef2cd1>:105) finished in 0.015 s\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(783)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Job 19 finished: sortByKey at <ipython-input-10-5997fdef2cd1>:105, took 0.026334 s\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Starting job: collect at <ipython-input-10-5997fdef2cd1>:105\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Registering RDD 447 (sortByKey at <ipython-input-10-5997fdef2cd1>:105)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Got job 20 (collect at <ipython-input-10-5997fdef2cd1>:105) with 2 output partitions\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Final stage: ResultStage 830 (collect at <ipython-input-10-5997fdef2cd1>:105)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 829)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 829)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting ShuffleMapStage 829 (PairwiseRDD[447] at sortByKey at <ipython-input-10-5997fdef2cd1>:105), which has no missing parents\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_69 stored as values in memory (estimated size 6.7 KB, free 389.2 KB)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 4.3 KB, free 393.5 KB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.143.133.233:45916 (size: 4.3 KB, free: 908.9 MB)\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 829 (PairwiseRDD[447] at sortByKey at <ipython-input-10-5997fdef2cd1>:105)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Adding task set 829.0 with 2 tasks\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 829.0 (TID 148, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1883 bytes)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 829.0 (TID 149, yp-spark-dal09-env5-0033, partition 1,NODE_LOCAL, 1883 bytes)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 4.3 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 4.3 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(829)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 829.0 (TID 149) in 22 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 829.0 (TID 148) in 25 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 829.0, whose tasks have all completed, from pool \n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: ShuffleMapStage 829 (sortByKey at <ipython-input-10-5997fdef2cd1>:105) finished in 0.025 s\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: looking for newly runnable stages\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: running: Set()\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: waiting: Set(ResultStage 830)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: failed: Set()\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting ResultStage 830 (PythonRDD[450] at collect at <ipython-input-10-5997fdef2cd1>:105), which has no missing parents\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(829)\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageSubmitted: stageId(830)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_70 stored as values in memory (estimated size 5.6 KB, free 399.1 KB)\n17/01/19 16:40:14 INFO spark.storage.MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 3.5 KB, free 402.6 KB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.143.133.233:45916 (size: 3.5 KB, free: 908.9 MB)\n17/01/19 16:40:14 INFO apache.spark.SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1006\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 830 (PythonRDD[450] at collect at <ipython-input-10-5997fdef2cd1>:105)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Adding task set 830.0 with 2 tasks\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 1.0 in stage 830.0 (TID 150, yp-spark-dal09-env5-0033, partition 1,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Starting task 0.0 in stage 830.0 (TID 151, yp-spark-dal09-env5-0024, partition 0,NODE_LOCAL, 1894 bytes)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on yp-spark-dal09-env5-0033:38602 (size: 3.5 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO spark.storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on yp-spark-dal09-env5-0024:38984 (size: 3.5 KB, free: 4.2 GB)\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 48 to yp-spark-dal09-env5-0033:53120\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMaster: Size of output statuses for shuffle 48 is 239 bytes\n17/01/19 16:40:14 INFO apache.spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 48 to yp-spark-dal09-env5-0024:35270\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 1.0 in stage 830.0 (TID 150) in 24 ms on yp-spark-dal09-env5-0033 (1/2)\n17/01/19 16:40:14 INFO spark.scheduler.TaskSetManager: Finished task 0.0 in stage 830.0 (TID 151) in 30 ms on yp-spark-dal09-env5-0024 (2/2)\n17/01/19 16:40:14 INFO cluster.ego.EGODeployScheduler: Removed TaskSet 830.0, whose tasks have all completed, from pool \n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: ResultStage 830 (collect at <ipython-input-10-5997fdef2cd1>:105) finished in 0.030 s\n17/01/19 16:40:14 INFO cluster.ego.EGOFineGrainedSchedulerBackend: onStageCompleted: stageId(830)\n17/01/19 16:40:14 INFO spark.scheduler.DAGScheduler: Job 20 finished: collect at <ipython-input-10-5997fdef2cd1>:105, took 0.074184 s\n17/01/19 16:40:15 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1484865599, 2017-01-19 16:40:15 CST]\n", 
                    "output_type": "stream"
                }
            ], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# look for our log output in the latest kernel log file\n! grep 'CloudantRecommender' $(ls -1 $HOME/logs/notebook/*pyspark* | sort -r | head -1)", 
            "execution_count": 14, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "17/01/19 16:37:35 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-19 16:37:35 CST]\r\n17/01/19 16:38:27 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-19 16:38:27 CST]\r\n17/01/19 16:39:00 INFO CloudantRecommender: [Found, 1000000, records in Cloudant]\r\n17/01/19 16:39:00 INFO CloudantRecommender: [Starting train model: , 2017-01-19 16:39:00 CST]\r\n17/01/19 16:39:49 INFO CloudantRecommender: [Finished train model: , 2017-01-19 16:39:49 CST]\r\n17/01/19 16:39:49 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-19 16:39:49 CST]\r\n17/01/19 16:39:59 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-19 16:39:59 CST]\r\n17/01/19 16:39:59 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1484865599]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-19 16:40:01 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-19 16:40:03 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-19 16:40:06 CST]\r\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-19 16:40:08 CST]\r\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-19 16:40:08 CST]\r\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-19 16:40:08 CST]\r\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-19 16:40:08 CST]\r\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-19 16:40:08 CST]\r\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-19 16:40:08 CST]\r\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-19 16:40:08 CST]\r\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-19 16:40:08 CST]\r\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-19 16:40:08 CST]\r\n17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-19 16:40:08 CST]\r\n17/01/19 16:40:09 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-19 16:40:09 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-19 16:40:11 CST]\r\n17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-19 16:40:13 CST]\r\n17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-19 16:40:13 CST]\r\n17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-19 16:40:13 CST]\r\n17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-19 16:40:13 CST]\r\n17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendationdb metadata record, {'timestamp_utc': '2017-01-19T22:40:13.807491', '_id': 'recommendation_metadata', 'latest_db': 'recommendationdb_1484865599'}]\r\n17/01/19 16:40:15 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1484865599, 2017-01-19 16:40:15 CST]\r\n", 
                    "output_type": "stream"
                }
            ], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "# look for our log output in all kernel log files\n! grep 'CloudantRecommender' $HOME/logs/notebook/*pyspark* ", 
            "execution_count": 15, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:37:35 INFO CloudantRecommender: [Starting load from Cloudant: , 2017-01-19 16:37:35 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:38:27 INFO CloudantRecommender: [Finished load from Cloudant: , 2017-01-19 16:38:27 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:39:00 INFO CloudantRecommender: [Found, 1000000, records in Cloudant]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:39:00 INFO CloudantRecommender: [Starting train model: , 2017-01-19 16:39:00 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:39:49 INFO CloudantRecommender: [Finished train model: , 2017-01-19 16:39:49 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:39:49 INFO CloudantRecommender: [Starting __get_top_recommendations: , 2017-01-19 16:39:49 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:39:59 INFO CloudantRecommender: [Finished __get_top_recommendations: , 2017-01-19 16:39:59 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:39:59 INFO CloudantRecommender: [Created new recommendations db, recommendationdb_1484865599]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 0, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 100, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 200, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 300, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 400, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 500, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 600, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 700, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 800, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 900, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 1000, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 1100, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:01 INFO CloudantRecommender: [Saved recommendations chunk, 1200, 2017-01-19 16:40:01 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1300, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1400, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1500, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1600, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1700, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1800, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 1900, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2000, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2100, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2200, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:03 INFO CloudantRecommender: [Saved recommendations chunk, 2300, 2017-01-19 16:40:03 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2400, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2500, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2600, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2700, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2800, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 2900, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3000, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3100, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3200, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3300, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:06 INFO CloudantRecommender: [Saved recommendations chunk, 3400, 2017-01-19 16:40:06 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3500, 2017-01-19 16:40:08 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3600, 2017-01-19 16:40:08 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3700, 2017-01-19 16:40:08 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3800, 2017-01-19 16:40:08 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 3900, 2017-01-19 16:40:08 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4000, 2017-01-19 16:40:08 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4100, 2017-01-19 16:40:08 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4200, 2017-01-19 16:40:08 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4300, 2017-01-19 16:40:08 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:08 INFO CloudantRecommender: [Saved recommendations chunk, 4400, 2017-01-19 16:40:08 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:09 INFO CloudantRecommender: [Saved recommendations chunk, 4500, 2017-01-19 16:40:09 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4600, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4700, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4800, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 4900, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5000, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5100, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5200, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5300, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5400, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5500, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:11 INFO CloudantRecommender: [Saved recommendations chunk, 5600, 2017-01-19 16:40:11 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 5700, 2017-01-19 16:40:13 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 5800, 2017-01-19 16:40:13 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 5900, 2017-01-19 16:40:13 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendations chunk, 6000, 2017-01-19 16:40:13 CST]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:13 INFO CloudantRecommender: [Saved recommendationdb metadata record, {'timestamp_utc': '2017-01-19T22:40:13.807491', '_id': 'recommendation_metadata', 'latest_db': 'recommendationdb_1484865599'}]\r\n/gpfs/fs01/user/sc07-a3c399a7caae2d-99fc3133bdbb/logs/notebook/kernel-pyspark-20170119_222801.log:17/01/19 16:40:15 INFO CloudantRecommender: [Saved recommendations to: , recommendationdb_1484865599, 2017-01-19 16:40:15 CST]\r\n", 
                    "output_type": "stream"
                }
            ], 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "name": "python2", 
            "language": "python", 
            "display_name": "Python 2 with Spark 1.6"
        }, 
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "version": "2.7.11"
        }
    }
}